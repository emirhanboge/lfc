{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"get_issues_from_commitmessages.ipynb","provenance":[],"mount_file_id":"1zvhrchp06VnYYu6tbfsjESLMQ3DgWHq8","authorship_tag":"ABX9TyPYzq263Ky2S7rq5zhJF6Vy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"8dAOLiLll93F","executionInfo":{"status":"ok","timestamp":1649678558815,"user_tz":-180,"elapsed":265,"user":{"displayName":"Emirhan Böge (Student)","userId":"01753922911255557941"}}},"outputs":[],"source":["import json\n","import re\n","import pandas as pd"]},{"cell_type":"code","source":["issues = pd.read_pickle(\"/content/drive/MyDrive/Learning from Contributions/pytorch_issue_data/pickles/issues2\")\n","pullr = pd.read_pickle(\"/content/drive/MyDrive/Learning from Contributions/pytorch_issue_data/pickles/pullr2\")"],"metadata":{"id":"TPm1bHqXmAIO","executionInfo":{"status":"ok","timestamp":1649678312320,"user_tz":-180,"elapsed":7971,"user":{"displayName":"Emirhan Böge (Student)","userId":"01753922911255557941"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["pullr.head(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"V6ucsf20mATU","executionInfo":{"status":"ok","timestamp":1649678502535,"user_tz":-180,"elapsed":282,"user":{"displayName":"Emirhan Böge (Student)","userId":"01753922911255557941"}},"outputId":"eb6cf35f-7dbd-40ef-dda5-00b8de54c2f3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      issue                   title         labels   state    author  \\\n","0  issue687  Add CrossEntropyLoss2d  [open source]  closed  desimone   \n","\n","  author_association            open_date           close_date  \\\n","0        CONTRIBUTOR  2017-02-05 19:09:11  2017-04-19 15:40:01   \n","\n","                                                body  likes  ...  \\\n","0  See #551\\r\\n- Add support for `CrossEntropyLos...      0  ...   \n","\n","                                                 sha  \\\n","0  [2f3c2bd685b9ca561c868319c532dfb8bbd63f8f, 54e...   \n","\n","                                      commit_message  \\\n","0  [Revert \"Add CrossEntropyLoss2d, PEP8\"\\n\\nThis...   \n","\n","                     commit_author  \\\n","0  [Bobby DeSimone, bdd, bdd, bdd]   \n","\n","                                  commit_author_date  \\\n","0  [2017-02-05T19:38:02Z, 2017-02-05T19:48:40Z, 2...   \n","\n","                         committer  \\\n","0  [Bobby DeSimone, bdd, bdd, bdd]   \n","\n","                                      committer_date  \\\n","0  [2017-02-05T19:38:02Z, 2017-02-05T19:48:40Z, 2...   \n","\n","                   verification                       verification_reason  \\\n","0  [False, False, False, False]  [unsigned, unsigned, unsigned, unsigned]   \n","\n","  release seconds_to_close  \n","0  v0.1.9        6294650.0  \n","\n","[1 rows x 27 columns]"],"text/html":["\n","  <div id=\"df-b27c1533-d303-4973-bb2b-3f574cdf6d02\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>issue</th>\n","      <th>title</th>\n","      <th>labels</th>\n","      <th>state</th>\n","      <th>author</th>\n","      <th>author_association</th>\n","      <th>open_date</th>\n","      <th>close_date</th>\n","      <th>body</th>\n","      <th>likes</th>\n","      <th>...</th>\n","      <th>sha</th>\n","      <th>commit_message</th>\n","      <th>commit_author</th>\n","      <th>commit_author_date</th>\n","      <th>committer</th>\n","      <th>committer_date</th>\n","      <th>verification</th>\n","      <th>verification_reason</th>\n","      <th>release</th>\n","      <th>seconds_to_close</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>issue687</td>\n","      <td>Add CrossEntropyLoss2d</td>\n","      <td>[open source]</td>\n","      <td>closed</td>\n","      <td>desimone</td>\n","      <td>CONTRIBUTOR</td>\n","      <td>2017-02-05 19:09:11</td>\n","      <td>2017-04-19 15:40:01</td>\n","      <td>See #551\\r\\n- Add support for `CrossEntropyLos...</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>[2f3c2bd685b9ca561c868319c532dfb8bbd63f8f, 54e...</td>\n","      <td>[Revert \"Add CrossEntropyLoss2d, PEP8\"\\n\\nThis...</td>\n","      <td>[Bobby DeSimone, bdd, bdd, bdd]</td>\n","      <td>[2017-02-05T19:38:02Z, 2017-02-05T19:48:40Z, 2...</td>\n","      <td>[Bobby DeSimone, bdd, bdd, bdd]</td>\n","      <td>[2017-02-05T19:38:02Z, 2017-02-05T19:48:40Z, 2...</td>\n","      <td>[False, False, False, False]</td>\n","      <td>[unsigned, unsigned, unsigned, unsigned]</td>\n","      <td>v0.1.9</td>\n","      <td>6294650.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1 rows × 27 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b27c1533-d303-4973-bb2b-3f574cdf6d02')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b27c1533-d303-4973-bb2b-3f574cdf6d02 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b27c1533-d303-4973-bb2b-3f574cdf6d02');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["regex = r\"([\\s]+|[(]+)#([^\\s]+|[(]+)\"\n","for index, row in pullr.iterrows():\n","    for x in row[\"commit_message\"]:\n","      matches = re.findall(regex, x)\n","      if matches != []:\n","          print(f\"Matches: {matches}\")\n","          print(x, \"\\n-----------------\")\n","          "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbA2Wv2ynek2","executionInfo":{"status":"ok","timestamp":1649682221920,"user_tz":-180,"elapsed":35257,"user":{"displayName":"Emirhan Böge (Student)","userId":"01753922911255557941"}},"outputId":"16dcce46-0d66-4997-e359-1c979f1dd37b"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix flake8 error. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Rename 'ITensorList' to 'ITensorListRef'. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix 'index.Tensor' test. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Remove stale comment. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix a few compiler errors. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Invoke constructor explicitly when translating. on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Unmerge `cat` and `Tensor?[]` support (mistakenly removed). on \"Change API type `Tensor[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","This PR:\n","- adds support for `ITensorList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `ITensorList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `ITensorList` instead of `ArrayRef<Tensor>`\n","\n","**Changes summary:**\n","\n","- Signature changes due to the different APIs:\n","  - dispatcher API (e.g. `BatchingRegistrations.cpp`)\n","  - C++ API (e.g. `TensorShape.cpp`)\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `ITensorList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Signature changes of `at::cat` due to the need of `const` inside `TensorBody.h`\n","- Forward declarations of `ITensorList` (e.g. `MethodOperators.h`)\n","- Codegen changes, special casing structured kernels (e.g. `gen.py`)\n","\n","**Short description of structured kernels special casing:**\n","\n","I introduced, mainly, 5 types of changes to the codegen for generating code depending on\n","whether the kernel is structured or not:\n","\n","1. Added a `structured_type_override` flag to the `argument_type` function definition of\n","the affected APIs (mainly the dispatcher and C++ APIs).\n","  - `api/cpp.py`, `api/dispatcher.py`, `api/native.py`\n","2. Added a `structured_type_override` member to the signature\n","classes (e.g. `CppSignature`), since `FunctionSchema` doesn't really know whether the\n","function is structured or not\n","  - `api/types.py`\n","3. Added a `part_of_structured_group` to `NativeFunction` class, which is just a\n","convenient function to forward to `structured_type_override` wherever needed\n","  - `model.py`\n","4. Appropriately changed the rest of the codegen, whenever it used either the signature\n","classes or the `arguments` function directly\n","5. Added a check for `const ITensorList&` type wherever there was a check for `TensorList`\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix 'index.Tensor' test. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix a few compiler errors. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Unmerge `cat` and `Tensor?[]` support (mistakenly removed). on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Rebased to 'viable/strict.' on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix flake8 errors. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Rename 'ITensorList' to 'ITensorListRef'. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Remove stale comment. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Change API type `Tensor?[]` for structured kernels.\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Rebased to viable/strict. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Invoke constructor explicitly when translating. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Fix flake8 error. on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '66328')]\n","Update on \"Change API type `Tensor?[]` for structured kernels.\"\n","\n","\n","Partially fixes: #66328\n","\n","(basically, same as previous PR, but for `IOptTensorRefList`)\n","\n","This PR:\n","- adds support for `IOptTensorRefList` to the dispatcher for:\n","  - computing the dispatch key\n","  - boxing and unboxing `IOptTensorRefList`\n","- modified the codegen for structured kernels:\n","  - codegen APIs use `IOptTensorRefList` instead of `List<optional<Tensor>>`\n","\n","**Changes summary:**\n","\n","- Miscelaneous functions used by codegen'd functions (e.g. `FunctionalTensorWrapper.*`)\n","- Dispatcher changes for handling `IOptTensorRefList` correctly (e.g. `DispatchKeyExtractor.h`)\n","- Forward declarations of `IOptTensorRefList` (e.g. `MethodOperators.h`)\n","- Signature changes of `at::index` due to the need of `const` inside `TensorBody.h`\n","- Codegen changes, special casing structured kernels (e.g. `gen_variable_type.py`)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '71746)')]\n","Introduce an environment variable to change c10 log level (#71746)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/71746\n","\n","This PR contains the following improvements:\n","\n","- It exposes a new environment variable `TORCH_CPP_LOG_LEVEL` that enables users to set the log level of c10 logging facility (supports both GLOG and c10 loggers). Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.\n","- It implements an `initLogging()` function and calls it as part of `torch._C` module import to ensure that the underlying logging facility is correctly initialized in Python.\n","\n","With these changes a user can dynamically set the log level of c10 as in the following example:\n","\n","```\n","$ TORCH_CPP_LOG_LEVEL=INFO python my_torch_script.py\n","```\n","ghstack-source-id: 149822703\n","\n","Test Plan: Run existing tests.\n","\n","Reviewed By: malfet\n","\n","Differential Revision: D33756252\n","\n","fbshipit-source-id: 7fd078c03a598595d992de0b474a23cec91838af\n","(cherry picked from commit 01d6ec6207faedf259ed1368730e9e197cb3e1c6) \n","-----------------\n","Matches: [('(', '73144)'), (' ', '73121')]\n","Prefix c10d log messages with `[c10d]` for easier troubleshooting (#73144)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73144\n","\n","This PR formats c10d log messages written by the `C10D_INFO/WARN/ERROR` macros by prefixing them with the `[c10d]` tag for easier troubleshooting. See #73121 for a specific customer request.\n","\n","Note though that this is a temporary fix to unblock our users. Ideally our global logging facility should natively support component-based preambles.\n","ghstack-source-id: 149748943\n","\n","Test Plan: N/A\n","\n","Reviewed By: rohan-varma\n","\n","Differential Revision: D34363975\n","\n","fbshipit-source-id: 6b8096ac4b2fa344406c866a2e7665541cb60b34\n","(cherry picked from commit af14aef18d0239f04730545596a05536e0f9c857) \n","-----------------\n","Matches: [('(', '73166)')]\n","Refactor TORCH_DISTRIBUTED_DEBUG implementation (#73166)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73166\n","\n","This PR refactors, cleans up, and optimizes the implementation of `TORCH_DISTRIBUTED_DEBUG`. It also introduces three new user APIs: `get_debug_level()`, `set_debug_level()`, and `set_debug_level_from_env()` to retrieve and modify the debug level after a process has started.\n","ghstack-source-id: 149778566\n","\n","Test Plan: Run the existing unit tests.\n","\n","Reviewed By: rohan-varma\n","\n","Differential Revision: D34371226\n","\n","fbshipit-source-id: e18443b411adcbaf39b2ec999178c198052fcd5b\n","(cherry picked from commit 26d6bb1584b83a0490d8b766482656a5887fa21d) \n","-----------------\n","Matches: [('(', '73149)')]\n","Make \"server socket not listening\" warning logs less noisy (#73149)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73149\n","\n","This PR improves the handling of the \"server socket not yet listening\" warning log in c10d `socket`. Instead of outputting it after every failed attempt (meaning every second), it is now written every 20 seconds. Note though that if the log level is set to `INFO`, we keep writing a detailed message every second as before with additional `errno` information.\n","\n","With log level set to `WARN` the output looks like:\n","```\n","[W socket.cpp:598] [c10d] No socket on (127.0.0.1, 29501) is listening yet, will retry.\n","[W socket.cpp:598] [c10d] No socket on (127.0.0.1, 29501) is listening yet, will retry.\n","...\n","[E socket.cpp:726] [c10d] The client socket has timed out after 300s while trying to connect to (127.0.0.1, 29501).\n","```\n","\n","With log level set to `INFO` (a.k.a. verbose or debug level) the output looks like:\n","```\n","[I socket.cpp:515] [c10d] The client socket will attempt to connect to an IPv6 address of (127.0.0.1, 29501).\n","[I socket.cpp:582] [c10d] The client socket is attempting to connect to [localhost]:29501.\n","[I socket.cpp:643] [c10d] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused), will retry.\n","[W socket.cpp:598] [c10d] No socket on (127.0.0.1, 29501) is listening yet, will retry.\n","[I socket.cpp:582] [c10d] The client socket is attempting to connect to [localhost]:29501.\n","[I socket.cpp:643] [c10d] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused), will retry.\n","[I socket.cpp:582] [c10d] The client socket is attempting to connect to [localhost]:29501.\n","[I socket.cpp:643] [c10d] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused), will retry.\n","[I socket.cpp:582] [c10d] The client socket is attempting to connect to [localhost]:29501.\n","[I socket.cpp:643] [c10d] The server socket on [localhost]:29501 is not yet listening (errno: 111 - Connection refused), will retry.\n","...\n","[W socket.cpp:598] [c10d] No socket on (127.0.0.1, 29501) is listening yet, will retry.\n","...\n","[E socket.cpp:726] [c10d] The client socket has timed out after 300s while trying to connect to (127.0.0.1, 29501).\n","```\n","ghstack-source-id: 149778565\n","\n","Test Plan: Run manual tests to verify the correctness of the log message.\n","\n","Reviewed By: rohan-varma\n","\n","Differential Revision: D34365217\n","\n","fbshipit-source-id: 296d01fa8b1ba803432903c10686d8a75145e539\n","(cherry picked from commit 8ae5aff0c5ffcc3e87d27d2deba6fedf8cef45cd) \n","-----------------\n","Matches: [('(', '73167)')]\n","Introduce debug and trace log levels in c10d (#73167)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73167\n","\n","This PR adds `C10D_DEBUG` and `C10D_TRACE` macros to enable fine grained logging in c10d. It also updates some log statements of `socket` to make its output less noisy.\n","ghstack-source-id: 149778567\n","\n","Test Plan: Manual testing with different socket conditions.\n","\n","Reviewed By: rohan-varma\n","\n","Differential Revision: D34371426\n","\n","fbshipit-source-id: a852b05ec353b18b0540ce5f803666c3da21ddd7\n","(cherry picked from commit 4519b06ac57f177dfc086bc10e8e1a746ba0870d) \n","-----------------\n","Matches: [('(', '72304)')]\n","Move CUDA linalg code to its own subfolder (#72304)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72304\n","\n","This is a no-op change that simply moves files around in preparation of moving linear algebra in its own dynamically boundable module\n","This also simplifies torch_cuda_cu build rules, as all files from linalg it needs are in its own folder now.\n","Bazel CUDA rules are in some weird disarray(needed to add wildcard there as it ignores files mentioned in build_variables.so) and similar wildcard needs to be added to internal build system.\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: dagitses, ngimel\n","\n","Differential Revision: D33992796\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: 3f4fa1c224016d03e1a982a7ae5ac7807bc772e2\n","(cherry picked from commit 6a5a1b0c3f306a8915a45bcdf2c51f15b02d8a14) \n","-----------------\n","Matches: [('(', '72306)')]\n","Add BUILD_LAZY_CUDA_LINALG option (#72306)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72306\n","\n","When enable, it will generate `torch_cuda_linalg` library, which would depend on cusolve and magma and registers dynamic bindings to it from LinearAlgebraStubs\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: ngimel\n","\n","Differential Revision: D33992795\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: d1fa351a320659b29754997c20d754e69bfe36c0\n","(cherry picked from commit d5d6c69a988b9454538ecd28674206da2541de17) \n","-----------------\n","Matches: [('(', '72340)')]\n","Add TORCH_CUDA_CU_API to CUDABlas functions (take 2) (#72340)\n","\n","Summary:\n","In order to make them accessible from other libraries\n","Also make `REGISTER_ARCH_DISPATCH` export dispatches as  TORCH_API, so that stubs could be called from libraries other than `torch_cpu`. To satisfy Windows builds, add the same `TORCH_API` to the static members declarations, although they are noops on Linux.\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72340\n","\n","Reviewed By: janeyx99\n","\n","Differential Revision: D34007756\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: 6dcc4e350920c72f8b1762a5018082f7aeec98e9\n","(cherry picked from commit 9c1f44df8a957d93cb25eabedc5f94889bb7a007) \n","-----------------\n","Matches: [('(', '72958)')]\n","Move implementation of CUDA error handling to Exceptions.cpp (#72958)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72958\n","\n","Please note, that it must not depend on any of the symbols from CUDA libraries\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: gchanan\n","\n","Differential Revision: D34296428\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: ec46f0b847db39977187a8439e941085fd1dc8f5\n","(cherry picked from commit 3918339e04aa9ff8ee8d18fed07633f943799978) \n","-----------------\n","Matches: [('(', '73058)')]\n","Move magma utils to its own header (#73058)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73058\n","\n","And keep it in cuda/linalg folder to make sure all MAGMA and CUSolver usage in codebase is restricted to linalg\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: suo\n","\n","Differential Revision: D34327978\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: dd4539a2a76bce68cced94fba943bf8a1155db1e\n","(cherry picked from commit 15d8c9b5dd0955b8c1dd60df7714778f809db8ac) \n","-----------------\n","Matches: [('(', '73125)')]\n","`scatter_reduce` documentation (#73125)\n","\n","Summary:\n","Reland of https://github.com/pytorch/pytorch/issues/68580 (which were milestoned for 1.11) plus partial revert of https://github.com/pytorch/pytorch/pull/72543\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73125\n","\n","Reviewed By: bdhirsh\n","\n","Differential Revision: D34355217\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: 325ecdeaf53183d653b44ee5e6e8839ceefd9200\n","(cherry picked from commit 71db31748a8adcd8f95d5faf04aaa454e9c4c760)\n","(cherry picked from commit cfb6c942fed64dbb81ccc4f14b2a6650123af2e1) \n","-----------------\n","Matches: [('(', '73368)')]\n","Make debug_pkl smaller by only emitting unique traces. (#73368)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73368\n","\n","debug_pkl file inside of pytorch's .pt file consists of a list of SourceRanges. Each SourceRange points to a Source which is a stack track, filename, and start, end numbers. Those are emitted in debug_pkl file as strings.\n","Since many SourceRange shares the same source, the string for trace can be deduped.\n","The newer format saves a set of unique traces in a tuple, then each SourceRange will save the offset of it's trace w.r.t. position in that tuple. (i.e. manually applying dictionary compression).\n","The above helps with smaller file size. On loading, if we copy each trace to Source as string the runtime memory would still blowup.\n","To mitigate this, we use SourceView directly instead of source which will take the reference of string inside of Deserializer and make that into string_view. This is safe because Deserializer is hold by Unpickler by shared_ptr, and Unpickler is also hold by shared_ptr by another Source object. That Source object will be alive during the model construction.\n","\n","Test Plan:\n","unit test\n","\n","Took original file (312271638_930.predictor.disagg.local); loaded with `torch.jit.load` save again with `torch.jit.save`. Unzip both, look at contents:\n","```\n","[qihan@devvm5585.vll0 ~]$ du archive -h\n","4.0K    archive/xl_model_weights\n","3.7M    archive/extra\n","8.0K    archive/code/__torch__/caffe2/torch/fb/model_transform/splitting\n","8.0K    archive/code/__torch__/caffe2/torch/fb/model_transform\n","8.0K    archive/code/__torch__/caffe2/torch/fb\n","8.0K    archive/code/__torch__/caffe2/torch\n","8.0K    archive/code/__torch__/caffe2\n","20M     archive/code/__torch__/torch/fx/graph_module\n","20M     archive/code/__torch__/torch/fx\n","8.0K    archive/code/__torch__/torch/classes\n","20M     archive/code/__torch__/torch\n","20M     archive/code/__torch__\n","20M     archive/code\n","2.7M    archive/constants\n","35M     archive\n","[qihan@devvm5585.vll0 ~]$ du resaved -h\n","4.0K    resaved/extra\n","8.0K    resaved/code/__torch__/caffe2/torch/fb/model_transform/splitting\n","8.0K    resaved/code/__torch__/caffe2/torch/fb/model_transform\n","8.0K    resaved/code/__torch__/caffe2/torch/fb\n","8.0K    resaved/code/__torch__/caffe2/torch\n","8.0K    resaved/code/__torch__/caffe2\n","1.3M    resaved/code/__torch__/torch/fx/graph_module\n","1.3M    resaved/code/__torch__/torch/fx\n","8.0K    resaved/code/__torch__/torch/classes\n","1.4M    resaved/code/__torch__/torch\n","1.4M    resaved/code/__torch__\n","1.4M    resaved/code\n","2.7M    resaved/constants\n","13M     resaved\n","[qihan@devvm5585.vll0 ~]$\n","```\n","\n","Reviewed By: gmagogsfm\n","\n","Differential Revision: D34455360\n","\n","fbshipit-source-id: 68ff29c257ef962654903d5321dcdefb469ac737 \n","-----------------\n","Matches: [(' ', '72405')]\n","Remove legacy tensor constructors for complex dtypes\n","\n","PR #72405 added four new types to the public python API:\n","`torch.ComplexFloatTensor`, `torch.ComplexDoubleTensor`,\n","`torch.cuda.ComplexFloatTensor` and `torch.cuda.ComplexDoubleTensor`.\n","\n","I believe this was unintentional and a clarifying comment as to the\n","purpose of `all_declared_types` is needed to avoid this in future.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73378)')]\n","T112685841: Use irange in PyTorch (#73378)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73378\n","\n","1) ran check_for_c10_loops.py to automatically update all files (*.h, *.hpp, *.cpp) under fbcode/caffe2/torch (this is the path in the check_for_c10_loops.py, slightly different from the task description where the path mentioned was fbcode/caffe2. since current commit already contains 27 files, will use a separate commit for additional files).\n","\n","2) manually reviewed each change, and reverted a few files:\n","    (a) select_keys.cpp, bucketize_calibration.cpp, index_mmh and TCPStore.cpp: iterator modified in loop\n","    (b) qlinear_4bit_ops.cpp and id_list_feature_merge_conversion.cpp: condition containing multiple expressions.\n","\n","Test Plan:\n","Doing the following (still in progress, will address issues as they appear):\n","buck build ...\n","buck test ...\n","\n","Reviewed By: r-barnes\n","\n","Differential Revision: D34435473\n","\n","fbshipit-source-id: f0b1b7aa30d5e43e131e17ebc943a89ddff15459 \n","-----------------\n","Matches: [('(', '73406)')]\n","[fx][acc_tracer] fix defaulted placeholder normalization (#73406)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73406\n","\n","Placeholder defaults are stored in `node.args`, during normalization we had dropped these. This diff passes the default args through the normalization transformation.\n","\n","Test Plan:\n","Added tests to cover cases with optional inputs, test covers\n","* nothing passed to optional input\n","* `None` passed to optional input\n","* a tensor passed to optional input\n","\n","Reviewed By: jfix71\n","\n","Differential Revision: D34463493\n","\n","fbshipit-source-id: 02bfa84229c0dd940d05a51d4c26e4c0c1f078c9 \n","-----------------\n","Matches: [('(', '73409)')]\n","[Profiler] Specialized AppendOnlyQueue (#73409)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73409\n","\n","We can do better than `vector` or `deque`, and it's sufficiently important to the hot path to justify a custom container. (This is part of the larger queue refactor, but this is a standalone drop-in replacement so we don't need to wait.)\n","\n","Test Plan: It's a pretty simple container type, so I just added a few cpp tests for emplace and read back. I also ran the overhead benchmark (replicates=9) with both `--stressTestKineto` (0.99 -> 0.94 us) and `--stressTestKineto --kinetoProfileMemory` (1.36 -> 1.27 us).\n","\n","Reviewed By: swolchok\n","\n","Differential Revision: D34231072\n","\n","fbshipit-source-id: fccbc9ab7adaa1645c631fee6aa85787f85df599 \n","-----------------\n","Matches: [(' ', '44459')]\n","Update on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Rebase and fix merge conflicts on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","Differential Revision: [D34571403](https://our.internmc.facebook.com/intern/diff/D34571403)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Update on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","Differential Revision: [D34571403](https://our.internmc.facebook.com/intern/diff/D34571403)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Update on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Add comments that python code will be removed on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","Differential Revision: [D34571403](https://our.internmc.facebook.com/intern/diff/D34571403)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Migrate _pad_circular to ATen\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '44459')]\n","Update on \"Implement _pad_circular in ATen\"\n","\n","\n","Closes #44459\n","\n","This migrates the python implementation of `_pad_circular` to ATen and\n","removes the old C++ implementation that had diverged from python.\n","\n","Note that `pad` can't actually use this until the\n","forward-compatibility period is over.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73414)')]\n","Don't use vector accessor methods to do pointer math; unblock platform010 (#73414)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73414\n","\n","The code here previously used this creative pointer math\n","```\n","const auto end = reinterpret_cast<uintptr_t>(&managed_tensor_storage_impls_.at(managed_tensor_storage_impls_.size()));\n","```\n","This has the form\n","```\n","const auto end = &A[N];\n","```\n","this works just fine if `A` is C-style array since `&A[N]` can get transformed to `(A+N)` where `A` is a simple pointer without ever dereferencing.\n","\n","But this is C++ and `A` is a vector, so `A[N]` calls the accessor method, reaches into an illegal place in memory, and then we get the address of that. (Or so I deduce.)\n","\n","We sidestep the issue by using `data()` to get the desired memory address directly.\n","\n","Test Plan: Sandcastle\n","\n","Differential Revision: D34468166\n","\n","fbshipit-source-id: 31ef34793dc7822c93384b018c81cbdf9e7f9816 \n","-----------------\n","Matches: [(' ', '73401')]\n","Rebase and fix merge conflicts on \"Remove F.pad python implementation\"\n","\n","\n","Closes #73401\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73401')]\n","Update on \"Remove F.pad python implementation\"\n","\n","\n","Closes #73401\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73401')]\n","Update on \"Remove F.pad python implementation\"\n","\n","\n","Closes #73401\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73401')]\n","Add comments that python code will be removed on \"Remove F.pad python implementation\"\n","\n","\n","Closes #73401\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73435)')]\n","Jit save/load meta tensors (#73435)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73435\n","\n","Add support for torch.jit.save and load for meta tensors to use in meta tensor based xl weights.\n","\n","Test Plan:\n","```\n","buck test //caffe2/test:jit && -- -r .*save_load_meta_tensors.*\n","```\n","\n","Reviewed By: houseroad\n","\n","Differential Revision: D34479511\n","\n","fbshipit-source-id: 29603c2df04288ddb697e2309e13c86e4bfd3036 \n","-----------------\n","Matches: [('(', '73439)')]\n","[torch] set workspace size for cublas lt interface 1M (#73439)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73439\n","\n","Per discussion in https://github.com/pytorch/pytorch/issues/73328#issuecomment-1050422698\n","Workspace size is needed to get good cublas lt performance\n","\n","Test Plan:\n","In PyTorch benchmark\n","python run.py nvidia_deeprecommender -d cuda -t train\n","\n","Reviewed By: xuzhao9\n","\n","Differential Revision: D34480690\n","\n","fbshipit-source-id: c6bf863164c9190cbbf7002233d8d84dd9af9372 \n","-----------------\n","Matches: [('(', '72124)')]\n","Port `amax` to structured kernel (#72124)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72124\n","\n","Reviewed By: bdhirsh\n","\n","Differential Revision: D34215708\n","\n","Pulled By: ansley\n","\n","fbshipit-source-id: fee887e331cb8bd9fab3d9d958ff13ac8d07be27\n","(cherry picked from commit 94dbb5b7e7e14a663dc02ecf5013fad10b8701b3) \n","-----------------\n","Matches: [('(', '69074)')]\n","codegen: do not generate code for dispatch_namespaced_definitions (#69074)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69074\n","\n","Reviewed By: jbschlosser\n","\n","Differential Revision: D32758621\n","\n","Pulled By: bdhirsh\n","\n","fbshipit-source-id: f8a174fd9d74039003f9713d8dfaae2b4eaa7089\n","(cherry picked from commit 462e92c82d196ba27228db4e7e1d8195a7f07e08) \n","-----------------\n","Matches: [('(', '72864)'), (' ', '72860')]\n","Fix asserts in tests (#72864)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72864\n","\n","Fixes #72860\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: rohan-varma\n","\n","Differential Revision: D34246987\n","\n","Pulled By: H-Huang\n","\n","fbshipit-source-id: 1ba47585533aff4cff9beec49bdc801f8320ffc8\n","(cherry picked from commit 03e45ceb890d72216950a9c3d5cd648b02e6a557) \n","-----------------\n","Matches: [('(', '72547)')]\n","[easy][PTE] Reduce unnecessary ref count bumps in callstack debug (#72547)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72547\n","\n","toTuple() returns a  new intrusive pointer that bumps its underlying ref count. Whereas, toTupeRef returns a reference. We can save an unnecessary ref count bump.\n","\n","Based on https://fb.workplace.com/groups/pytorch.edge.team/permalink/1021780808376658/\n","\n","similar to D34047666 (https://github.com/pytorch/pytorch/commit/85d7e73a8aa2dd74970017d11c7411b36b89dfc4)\n","ghstack-source-id: 148665193\n","\n","Test Plan:\n","```\n","> Executing task: buck: buck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide <\n","\n","Executing in directory: /data/users/pavithran/fbsource\n","buck test //xplat/caffe2:test_lite_interpreter  --config client.id=nuclide\n","\n","clang-9: warning: argument unused during compilation: '-pthread' [-Wunused-command-line-argument]\n","\n","Parsing buck files: finished in 2.1 sec\n","Creating action graph: finished in 0.5 sec\n","[RE] Metadata: Session ID=[reSessionID-66858379-0761-4966-a933-bc7f0d0add95]\n","[RE] Waiting on 0 remote actions. Completed 523 actions remotely, action cache hit rate: 0.00%.\n","Downloaded 3947/5089 artifacts, 20.92 Mbytes, 12.5% cache miss (for updated rules)\n","Building: finished in 01:04.0 min (100%) 5438/5438 jobs, 5192/5438 updated\n","  Total time: 01:06.6 min\n","Testing: finished in 06:53.7 min (71 PASS/0 FAIL)\n","BUILD SUCCEEDED\n","RESULTS FOR //xplat/caffe2:test_lite_interpreter\n","PASS    406.0s 71 Passed   0 Skipped   0 Failed   //xplat/caffe2:test_lite_interpreter\n","TESTS PASSED\n","\n","Terminal will be reused by tasks, press any key to close it.\n","```\n","\n","Reviewed By: kimishpatel\n","\n","Differential Revision: D34082609\n","\n","fbshipit-source-id: 4bcbdb2d11dd4c3bc392010487dccd2270278222\n","(cherry picked from commit dd64eb386d02335e566fb6496f2ff00a8879ccc3) \n","-----------------\n","Matches: [('(', '72886)')]\n","[Join][BE] Fix typo; remove obsolete method (#72886)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72886\n","\n","**Test Plan**\n","Searching for `_schedule_shadow_all_reduce_for_fwd_pass` shows that it is defined but never used.\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: mrshenli\n","\n","Differential Revision: D34255651\n","\n","Pulled By: awgu\n","\n","fbshipit-source-id: 205a0325c2cdc05e127a183cb86fa2fc2e0db99d\n","(cherry picked from commit 4492f03a3f37c01efa281a6d09a7e3b673cb1139) \n","-----------------\n","Matches: [('(', '72465)')]\n","[PyTorch] MHA: fix contiguity assumption in transform_bias_rescale_qkv (#72465)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72465\n","\n","This code path incorrectly assumed input tensors were contiguous. Now we check that.\n","ghstack-source-id: 149201476\n","\n","Test Plan: CI\n","\n","Reviewed By: ngimel\n","\n","Differential Revision: D34007665\n","\n","fbshipit-source-id: c43438f2495e32304ea3f7846e01eceb4a9448f7\n","(cherry picked from commit 0767b225f23846c1636ac3622f46b0c5ec071d96) \n","-----------------\n","Matches: [('(', '72464)')]\n","[PyTorch] MHA: add test for transform_bias_rescale_qkv (#72464)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72464\n","\n","We had some trouble getting this component (and this test!) right, so let's test it.\n","ghstack-source-id: 149201478\n","\n","Test Plan: new test passes\n","\n","Reviewed By: zrphercule\n","\n","Differential Revision: D33992477\n","\n","fbshipit-source-id: cc377eed5d4a4412b42bdabf360601c6e52947cf\n","(cherry picked from commit 9832867b12e555b512ded16decbea17b1794bda8) \n","-----------------\n","Matches: [('(', '72671)')]\n","[PyTorch] Handle non-vectorizable parameters for native MHA CUDA rescale kernel (#72671)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72671\n","\n","The existing kernel did not handle cases where D % 4 != 0 or dim_per_head % 4 != 0. Now we have a non-vectorized kernel for these cases.\n","ghstack-source-id: 149201477\n","\n","Test Plan: Updated test_nn to cover these cases.\n","\n","Reviewed By: zrphercule, ngimel\n","\n","Differential Revision: D34119371\n","\n","fbshipit-source-id: 4e9b4d9b636224ef2c433593f6f236df040de782\n","(cherry picked from commit f5393878e4c16342ee62465bb656b18053000677) \n","-----------------\n","Matches: [('(', '72059)')]\n","Improve numerical stability of `torch.distributions.wishart.Wishart` (#72059)\n","\n","Summary:\n","Maintanance of https://github.com/pytorch/pytorch/issues/70377\n","Multiple modifications of the merged initial implementation of Wishart distribution.\n","cc neerajprad\n","\n","Key modifications:\n","- `torch/distributions/wishart.py`: Clamp (Clip) float type values to calculate reciprocal in numerically stable manner, by using the `eps` value paired to each `torch.dtype`\n","- `test/distributions/test_distributions.py`: Test Wishart distribution implementation in numerically unstable zones, i.e `df` values are at `ndim - 1 < df < ndim` where `ndim` is the one dimenstion of the Wishart parameter & sample matrix.\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72059\n","\n","Reviewed By: H-Huang\n","\n","Differential Revision: D34245091\n","\n","Pulled By: neerajprad\n","\n","fbshipit-source-id: 1cd653c1d5c663346433e84fd0bbe2e590790908\n","(cherry picked from commit ef1da3ba465247f5777c3c40a90b96955c4281d0) \n","-----------------\n","Matches: [('(', '72847)')]\n","Fix command example (#72847)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72847\n","\n","Reviewed By: malfet\n","\n","Differential Revision: D34260868\n","\n","Pulled By: kit1980\n","\n","fbshipit-source-id: 1b225f3c2c7a822e44df4bbd91766e6533eab6d7\n","(cherry picked from commit c9e874c4d81a8f9ceee820e243c47b47a4361320) \n","-----------------\n","Matches: [('(', '72829)')]\n","[DataPipe] Improve .pyi generation (#72829)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72829\n","\n","Make two functions more flexible and usable from a different repo.\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: ejguan\n","\n","Differential Revision: D34227912\n","\n","Pulled By: NivekT\n","\n","fbshipit-source-id: 873934ed33caf485de7f56e9c4a1d3f3fa1a92ef\n","(cherry picked from commit b990c5e4c7244e1d0352b24c475df2c0968ee1c0) \n","-----------------\n","Matches: [('(', '72730)')]\n","Update lazy_ir.py from lazy_tensor_staging (#72730)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72730\n","\n","This diff contains changes from several PRs landed to lazy_tensor_staging branch.\n","- generating 'fallback' overrides for each codegenned op, useful for debugging\n","- supports operators which are missing aten:: symbols for op names, instead using their string counterpart\n","- makes the IR class a base class instead of hardcoding the assumption of TS\n","\n","Test Plan: tested on lazy_tensor_staging branch\n","\n","Reviewed By: desertfire\n","\n","Differential Revision: D34178476\n","\n","fbshipit-source-id: 7190b2e0d82b4eb1f4510c858c24446c6df3f9d0\n","(cherry picked from commit 6713d3f0ef1bb0ea5e2acfad2ea252cf73d1cbbf) \n","-----------------\n","Matches: [('(', '72899)')]\n","(2/2) Make TorchScript Preserve Fully Qualified Class Name for Python Exceptions: frontend change (#72899)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72899\n","\n","Reland D33282878 (https://github.com/pytorch/pytorch/commit/911d527b870bb4371da39be0c18a1ce109acb1d5). This is the frontend change.\n","ghstack-source-id: 149204031\n","\n","Test Plan: Refer to D33282878 (https://github.com/pytorch/pytorch/commit/911d527b870bb4371da39be0c18a1ce109acb1d5). Also check CI\n","\n","Reviewed By: gmagogsfm\n","\n","Differential Revision: D34252127\n","\n","fbshipit-source-id: 27b17ddd4d05d904eb91fd9ee094d9121f00e388\n","(cherry picked from commit 1d276baca308110ac40111ccd622400b3bbdc864) \n","-----------------\n","Matches: [('(', '72523)')]\n","[Perf] Reduce unnecessary ref count bumps (#72523)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72523\n","\n","`toTuple()` returns a  new intrusive pointer that bumps its underlying ref count. Whereas, `toTupeRef` returns a reference. We can save an unnecessary ref count bump.\n","ghstack-source-id: 149173308\n","\n","Test Plan: Sandcastle CI\n","\n","Reviewed By: swolchok\n","\n","Differential Revision: D34047666\n","\n","fbshipit-source-id: 8c821e45f7af4f3f1d098871926b9df288e329fb\n","(cherry picked from commit 34797e508d533c578a40f74ffc82b34e1c3ea40e) \n","-----------------\n","Matches: [('(', '72873)')]\n","[nnc][aot_compiler] Memory formats args to aot_compiler (#72873)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72873\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: priyaramani\n","\n","Differential Revision: D34250984\n","\n","Pulled By: IvanKobzarev\n","\n","fbshipit-source-id: e723ee64b024883eef78853e1b185b7040cafb09\n","(cherry picked from commit e9908df045acf33aa3cd0aec6784f15421236787) \n","-----------------\n","Matches: [('(', '72733)')]\n","[PT-D][Sharded Tensor] new init api for local tensor and sharding spec auto inference (#72733)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72733\n","\n","To improve the perf cost due to communication in the process of init the sharded tensor. There are two changes in this PR/diff:\n","\n","1. We create a new API named `_init_from_local_tensor` so that if we have only one local tensor, we can initiate a sharded tensor directly from it. (GH issue: https://github.com/pytorch/pytorch/issues/72092)\n","2. We create a new API to infer the sharding spec from global meta data, so we don't have to manually set the sharding spec when it's not `EnumerableShardingSpec`. (GH issue: https://github.com/pytorch/pytorch/issues/67244)\n","ghstack-source-id: 149229259\n","\n","Test Plan: CI\n","\n","Reviewed By: wanchaol\n","\n","Differential Revision: D34132739\n","\n","fbshipit-source-id: 3a60135761bcc19d6020b6c45cb2979869645ce6\n","(cherry picked from commit af569325e2794309a4a86e51749642a062a25f6e) \n","-----------------\n","Matches: [('(', '72496)')]\n","Create a CI workflow for XLA tests using the XLA test image (#72496)\n","\n","Summary:\n","This PR resolves https://github.com/pytorch/pytorch/issues/72693\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72496\n","\n","Reviewed By: H-Huang\n","\n","Differential Revision: D34255441\n","\n","Pulled By: seemethere\n","\n","fbshipit-source-id: fdfd54fbd59ef7266a78c9f729c1d5b6ed25e9d6\n","(cherry picked from commit ba14f0ee6cfa2fe248784d2dc5d54e427aef6bf7) \n","-----------------\n","Matches: [('(', '72806)')]\n","Set `BLAS_LIBRARIES` to `${MKL_LIBRARIES}` for MKL case (#72806)\n","\n","This reverts [suggestion](https://github.com/pytorch/pytorch/pull/49647#discussion_r677737470) proposed to https://github.com/pytorch/pytorch/pull/49647\n","\n","Which is somehow sufficient to workaround symptoms of https://github.com/pytorch/pytorch/issue/72653 \n","\n","I.e. before this change, `BLAS_LIBRARIES` were set to `caffe2::mkl`\n","which is an interface library with link property set as follows:\n","https://github.com/pytorch/pytorch/blob/59dd84cab6ede977173cd48d64abf1bcf6b2fabb/cmake/public/mkl.cmake#L10-L12 \n","-----------------\n","Matches: [('(', '72600)')]\n","[FSDP] Implement apply() (#72600)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72600\n","\n","Implements `apply()` which applies a `callable` of signature `f(m: Module) -> None` recursively to every submodule. The main difference from `nn.module.apply` is that this version summons the full parameters before apply() so it works appropriately with FSDP.\n","ghstack-source-id: 149217423\n","\n","Test Plan: CI\n","\n","Reviewed By: zhaojuanmao\n","\n","Differential Revision: D34111109\n","\n","fbshipit-source-id: 60d9d3f5c4d6c27763f5d68728dfb0bae3d9f644\n","(cherry picked from commit b20c65e06070f27fda0e5260f5cbbb41e3e33f46) \n","-----------------\n","Matches: [('(', '73445)'), (' ', '73429')]\n","Use intrusive_ptr for LazyTensor (#73445)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73445\n","\n","Refactors the whole codebase to use LazyTensorPtr (defined as c10::intrusive_ptr) to enable XLA to use a derived class XlaLazyTensor and override functionality.\n","\n","this PR is just the first step, and we will need to add a factory class that XLA can override in their backend to actually hook up their derived tensor class.\n","\n","Parallel PR on lazy_tensor_staging: #73429\n","\n","Test Plan: tested via lazy_tensor_staging test_ptltc and torchbench and CI\n","\n","Differential Revision: D34481918\n","\n","fbshipit-source-id: 0bd874a530e3dd73affe206911d7b907239bf839 \n","-----------------\n","Matches: [('(', '72306)')]\n","Add BUILD_LAZY_CUDA_LINALG option (#72306)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/72306\n","\n","When enable, it will generate `torch_cuda_linalg` library, which would depend on cusolve and magma and registers dynamic bindings to it from LinearAlgebraStubs\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: ngimel\n","\n","Differential Revision: D33992795\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: d1fa351a320659b29754997c20d754e69bfe36c0\n","(cherry picked from commit d5d6c69a988b9454538ecd28674206da2541de17) \n","-----------------\n","Matches: [('(', '548)'), (' ', '##'), (' ', '##')]\n","[libkineto] Re-enable user-annotations on GPU timeline (#548)\n","\n","Summary:\n","X-link: https://github.com/pytorch/kineto/pull/548\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73451\n","\n","Re-submitting Re-enable user-annotations on GPU timeline diff from Gisle.\n","\n","User annotations was previously pushed down to the GPU timelines but was disabled during a refactoring some time back.\n","This patch re-enables it internally at FB, and also enables it for the PyTorch profiler.\n","\n","Test Plan:\n","Added unit test.\n","\n","Internal Commands:\n","\n","  buck test mode/opt kineto/libkineto:\n","\n","  buck run mode/opt caffe2/benchmarks/fastrnns:bench -- --cnns resnet50 --group cnns --nloops 10000\n","\n","  dyno gputrace\n","\n","Shows ### forward ### user annotations now:\n","{F679365381}\n","https://www.internalfb.com/intern/perfdoctor/trace_view?filepath=tree%2Ftraces%2Fdynocli%2F0%2F1645817801%2F127.0.0.1%2Flibkineto_activities_1146310.json.gz&bucket=gpu_traces\n","\n","Reviewed By: briancoutinho\n","\n","Differential Revision: D32313588\n","\n","Pulled By: aaronenyeshi\n","\n","fbshipit-source-id: 501a63ca1721e6c716046848d2432b11594a98cd \n","-----------------\n","Matches: [('(', '73452)')]\n","fused int8 static (#73452)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73452\n","\n","Added a fused Int8FC path using PackAWithQuantRowOffset like the INT8 dynamic path. There are two ways to enable it\n","(1) set an positive \"X_scale\" value in the arg list of Int8FC op\n","(2) send both \"Qparam\" (for output requantization, could be dummy values) and \"in_Qparam\" (for fused input quantization)\n","\n","Differential Revision: D34034681\n","\n","fbshipit-source-id: b80b7b165fc684370d5d7a977b327955b599091a \n","-----------------\n","Matches: [('(', '73455)')]\n","Back out \"[pytorch] use cublas lt interface for bias fusion\" (#73455)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73455\n","\n","Original commit changeset: cecc51a27f4b\n","\n","Original Phabricator Diff: D33928147 (https://github.com/pytorch/pytorch/commit/3aecce70152579a01e969b8ea1540f301290fe5a)\n","\n","We are seeing a couple of issues with cublas lt. Sending this diff in case we need to revert.\n","\n","For example, we're observing\n","(1000x10) * (10x10) + bias(10) works.\n","(655360x10) * (10x10) + bias(10) fails.\n","disabling cublasLt, (655360x10) * (10x10) + bias(10) works\n","\n","Differential Revision: D34486240\n","\n","fbshipit-source-id: 3cd44d5d5d650c483beee751bd8047a13d3683e3 \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '65018')]\n","Update on \"Support for tensor subclasses as parameters\"\n","\n","\n","Fixes #65018\n","\n","This PR adds support for tensor subclasses as module parameters. It takes the approach outlined [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1035059173) of \"tagging\" a subclass instance with a flag (`_is_param`) to indicate it is a parameter. Both wrapper and non-wrapper subclasses are supported. Changes are as follows:\n","* `torch/nn/parameter.py`: Expand `nn.Parameter` to insert the new attribute onto tensor subclasses. Override `__instancecheck__` so that `isinstance(t, Parameter)` returns `True` when the attribute is present (the core expected semantic for parameters).\n","* `torch/_tensor.py` / `torch/_tensor_str.py`: as mentioned [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050060426), `__repr__()` causes some unique problems. Combining the solutions described [here](https://github.com/pytorch/pytorch/issues/65018#issuecomment-1050195314), the tweaks in these files add a new `tensor_contents` option to the base `Tensor.__repr__` that allow the tensor content component to be customized. Alternative suggestions on the nicest way to do this are welcome!\n","\n","The UX for this matches that for plain tensors:\n","```python\n","class MyModule(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.param1 = nn.Parameter(MyTensor(...))\n","        self.register_parameter('param2', nn.Parameter(MyTensor(...)))\n","    ...\n","```\n","Parameter versions of custom tensors are still the type of that custom tensor and can be used as expected.\n","\n","Additionally, it introduces new testing logic:\n","* `test/test_subclass.py`: validates semantics of subclasses as parameters for all common use cases. The ideal future state could be to make existing test coverage runnable with a test set of tensor subclasses, but that's quite a bit of work, so this new test file exists in the meantime. Current coverage here includes:\n","    * Deepcopy (should propagate parameter-ness)\n","    * Serialization (should propagate parameter-ness)\n","    * Training with optimizers\n","    * String representation (should include parameter-ness in repr)\n","    * Module parametrization\n","    * Type propagation (parameter-ness should not be propagated, but subclass types generally should be)\n","    * Lazy modules (doesn't work yet! set as `expectedFailure`)\n","* `torch/testing/_internal/common_subclass.py`: introduces a database of tensor subclasses to run the above set of tests on. Currently, there are 4 tensor subclass types being run through the tests, most of which were lifted from the Colab:\n","    * `LoggingTensor` (with a small change made to its `__repr__`)\n","    * `SparseTensor` - a wrapper tensor maintaining values and indices\n","    * `NonWrapperTensor` - a non-wrapper tensor\n","    * `DiagTensorBelow` - a simple wrapper tensor below autograd\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73464)')]\n","[fx/graph_drawer] Add args/kwargs and users (#73464)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73464\n","\n","- Improve formatting of graph by centering everything\n","- Add num_users\n","- Add args/kwargs\n","  - Don't print more than 10 of any list/tuple by default (this is necessary for very large concats)\n","\n","Test Plan: tested locally\n","\n","Differential Revision: D34492256\n","\n","fbshipit-source-id: 35bc063ff65fda42c8279b0bec2d13627dcc6a6b \n","-----------------\n","Matches: [('(', '73476)\"')]\n","Revert \"[test] Disable TestXNNPACK on ROCM machines (#73476)\"\n","\n","This reverts commit 09697c6df57c05dfd73ebdf02c87569437a00b71. \n","-----------------\n","Matches: [('(', '73476)')]\n","[test] Disable TestXNNPACK on ROCM machines (#73476)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73476\n","\n","This is a temp fix, and needs further investigation.\n","\n","Test Plan: CI\n","\n","Differential Revision: D34501632\n","\n","fbshipit-source-id: 75303f64febad8627ef4d426f53bfea61b6c3a9e \n","-----------------\n","Matches: [('(', '73477)')]\n","(torch/elastic) skip logging structured error info if error_file is not set (#73477)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73477\n","\n","resolves https://github.com/pytorch/pytorch/issues/73465\n","\n","This `log.error` is not necessary (and its also not human-friendly formatted) because we end up re-raising the same exception after recording the exception into an error_file (if present). Eventually python should handle this error the way it handles any other errors and will write the trace info into the console. This additional logging produces duplicate error console prints, which affects all users whose schedulers do not set `TORCHELASTIC_ERROR_FILE` env var when calling `torch.distributed.run`.\n","\n","Test Plan:\n","Induce an error on the agent process by `kill -15 $AGENT_PID`\n","```\n","python -m torch.distributed.run \\\n","   --nproc_per_node 2 \\\n","   --nnodes 1:1 \\\n","   --rdzv_backend c10d \\\n","  --rdzv_endpoint localhost:29500 \\\n","  --monitor_interval 3 test.py\n","```\n","\n","Produces\n","\n","{F704936697}\n","\n","In contrast to the duplicated error before:\n","\n","{F704936729}\n","\n","Reviewed By: d4l3k\n","\n","Differential Revision: D34501852\n","\n","fbshipit-source-id: 5112306ef2d242cc95330a28dc5de7a23a9c0b0d \n","-----------------\n","Matches: [('(', '73422)')]\n","Also install c10d headers with .h extension (#73422)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73422\n","\n","Fixes https://github.com/pytorch/pytorch/issues/73421\n","ghstack-source-id: 149978120\n","\n","Test Plan: None\n","\n","Reviewed By: cbalioglu\n","\n","Differential Revision: D34475711\n","\n","fbshipit-source-id: 9e4d1d57021cbff51f53762b32bbfffbf3f81c4c \n","-----------------\n","Matches: [('(', '73525)')]\n","[pytorch] disable bias fusion using cublas lt (#73525)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73525\n","\n","https://github.com/pytorch/pytorch/issues/73328 for context\n","\n","Test Plan: CI\n","\n","Reviewed By: jasonjk-park\n","\n","Differential Revision: D34528523\n","\n","fbshipit-source-id: 8bbda09d0cc8a0a388df31d6ce2eb8feb4a4b8db \n","-----------------\n","Matches: [('(', '73536)')]\n","[Static Runtime] Introduce StaticNodeInfo to store ProcessedNode's data independent from runtime instances (#73536)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73536\n","\n","Currently `StaticNodeInfo` class assumes 2 distinct roles that are not too obvious:\n","\n","1) \"template\" that contains metadata of an actual executable node by runtime. owned by `StaticModule`\n","\n","2) fully instanced ones that are owned by `StaticRuntime`.\n","\n","We currently merge these two usecases into one class, that can be error-prone in case illegal copying happens uncontrollably. Currently, we only copy objects of kind (1) into objects of kind (2) when a `StaticRuntime` instance is created.\n","\n","To address ths issue, this change introduces `StaticNodeInfo`, a separate class, to distinguishes the aforementioned two usecases in the code more clearly. With this `StaticNodeInfo` is for (1) and `ProcessedNode` is now for (2).\n","\n","Test Plan: Existing tests\n","\n","Reviewed By: mikeiovine\n","\n","Differential Revision: D33985600\n","\n","fbshipit-source-id: 8297b43902c3a8492c1ea51476dcd4fcfc7e3701 \n","-----------------\n","Matches: [('(', '73540)')]\n","Reserve the memory for vector to save cost in `gather_ranges_to_dense_op.h` (#73540)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/73540\n","\n","Reviewed By: r-barnes\n","\n","Differential Revision: D34492500\n","\n","fbshipit-source-id: 1cf04ef655465b5bfd05f0f6c90d6160dd05bb43 \n","-----------------\n","Matches: [('(', '73564)')]\n","[PyTorch FX] Return mapping of qualified names from split_module() (#73564)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73564\n","\n","While maintaining API backward compatibility, add an optional output parameter to split_module() that returns a mapping from the new qualified names in the modules after split to the old qualified names in the original module\n","\n","Test Plan:\n","1. Added a test (test_split_qualname_mapping) to test_fx_experimental.py to check the returned qualname mapping\n","```\n","$ python test_fx_experimental.py\n","...\n","Ran 1084 tests in 73.464s\n","OK (skipped=531, expected failures=4)\n","```\n","2. Ask test_fx.py to accept split_module's new signature\n","```\n","$ python test_fx.py --accept\n","```\n","\n","Differential Revision: D34541792\n","\n","fbshipit-source-id: f9b539b22ab81bd1271187af182e03d35d6768d8 \n","-----------------\n","Matches: [('(', '73540)')]\n","Reserve the memory for vector to save cost in `gather_ranges_to_dense_op.h` (#73540)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/73540\n","\n","Reviewed By: r-barnes\n","\n","Differential Revision: D34492500\n","\n","fbshipit-source-id: 1cf04ef655465b5bfd05f0f6c90d6160dd05bb43 \n","-----------------\n","Matches: [('(', '73598)')]\n","(torch/elastic) add documentation clarifying that torchrun is a console script to torch.distributed.run (#73598)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73598\n","\n","resolves https://github.com/pytorch/pytorch/issues/73319\n","\n","Simply clarifies that `torchrun` is a console script that invokes `python -m torch.distributed.run`.\n","\n","Test Plan: N/A doc change only, letting github CI validate that the docs build correctly.\n","\n","Reviewed By: sinannasir, d4l3k\n","\n","Differential Revision: D34558538\n","\n","fbshipit-source-id: 92a91c455bbbae7bf990266225bfde5b8fa5741d \n","-----------------\n","Matches: [('(', '73599)')]\n","[Vulkan] Optimize GRU operator with pre-packing (#73599)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73599\n","\n","Optimized GRU operator by using pre-packing for weights and biases in the Vulkan GPU backend:\n","* The weights and biases are always on the CPU side by design.\n","* To reduce the overhead by retrieving the weight and bias tensors every time, it is the best way to store them by pre-packing.\n","    * A custom op context `GruOpContext` (derived from `torch::jit::CustomClassHolder`) is created to hold both packed and unpacked data. It corresponds to the unpacked_ struct which represents the data needed to construct the op context. This data will be pre-packed and be stored in the packed_ struct. The constructor of the `GruOpContext` loads the data into the unpacked_ and packed_ structs.\n","    * `at::native::vulkan::ops::gru_prepack` and `at::native::vulkan::ops::gru_run` methods use the op context. The `gru_prepack` takes in whatever data is needed to construct the op context and returns a pointer to a created context. The `gru_run` takes input tensors and a pointer to the op context that uses the data stored in the context to process the inputs.\n","    * Lastly, we need to register the op context class and ops in [Register.cpp](https://github.com/pytorch/pytorch/blob/11dc1581298c5bb2b322897c7b3999d1a3971720/aten/src/ATen/native/vulkan/ops/Register.cpp). And rewrite the subgraph function of GRU op in [vulkan_rewrite.cpp](https://github.com/pytorch/pytorch/blob/11dc1581298c5bb2b322897c7b3999d1a3971720/torch/csrc/jit/passes/vulkan_rewrite.cpp) so that `gru_prepack` and `gru_run` ops can be executed instead in the Vulkan GPU backend.\n","* To avoid `\"Undefined symbols for architecture x86_64\"` compiler error on the x86_64 platform, `c10::Dispatcher::callBoxed()` API is used to call `vulkan_prepack::gru_prepack` and `vulkan_prepack::gru_run` by name. Otherwise, the test methods can't resolve the symbols.\n","* Added new tests for the GRU pre-packing and run operations: `gru_prepack_success` and gru_prepack_invalidinputs_exceptions`\n","* To build your PyTorch OSS on your local machine:\n","```\n","python setup.py clean\n","git submodule update --init --recursive\n","USE_VULKAN=1 USE_VULKAN_FP16_INFERENCE=1 python3 setup.py install --cmake\n","python setup.py develop && python -c \"import torch\"\n","```\n","* To run and dump a model containing GRU operators in Python:\n","```\n","import torch\n","from torch.utils import mobile_optimizer\n","model = torch.jit.load(\"Mclaren_traced.pt\")\n","vk_model = mobile_optimizer.optimize_for_mobile(model, backend=\"vulkan\")\n","print(vk_model.graph)\n","```\n","* The following torch scripts are the updated version by GRU pre-packing:\n","```\n","%15 : Tensor[] = prim::ListConstruct(%weight_ih_l0.1, %weight_hh_l0.1, %bias_ih_l0.1, %bias_hh_l0.1, %weight_ih_l1.1, %weight_hh_l1.1, %bias_ih_l1.1, %bias_hh_l1.1)\n","%19 : __torch__.torch.classes.vulkan.GruOpContext = vulkan_prepack::gru_prepack(%15, %4, %5, %6, %3, %3, %4)\n","%20 : Tensor, %21 : Tensor = vulkan_prepack::gru_run(%input.1, %hx.1, %19)\n","%18 : (Tensor, Tensor) = prim::TupleConstruct(%21, %20)\n","return (%18)\n","```\n","* This implementation has some limitations:\n","    * Tensor dim should be 3 for input sequence and hidden state.\n","    * has_biases=True\n","    * train=False\n","    * bidirectional=False\n","    * batch_first=True\n","    * dropout=0.0\n","    * D=1 since bidirectional=False\n","    * N=1 (batch size)\n","    * L=1 (sequence length)\n","\n","Test Plan:\n","Build & test on Android:\n","```\n","cd ~/fbsource\n","buck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:pt_vulkan_api_test_binAndroid\\#android-arm64 --show-output\n","adb push buck-out/gen/xplat/caffe2/pt_vulkan_api_test_binAndroid\\#android-arm64 /data/local/tmp/vulkan_api_test\n","adb shell \"/data/local/tmp/vulkan_api_test\"\n","```\n","Build & test on MacOS (x86_64):\n","```\n","cd ~/fbsource\n","buck build //xplat/caffe2:pt_vulkan_api_test_binAppleMac\n","./buck-out/gen/xplat/caffe2/pt_vulkan_api_test_binAppleMac\\#macosx-x86_64\n","```\n","\n","Test result on Android (Google Pixel 5):\n","```\n","Running main() from gtest_main.cc\n","[==========] Running 4 tests from 1 test case.\n","[----------] Global test environment set-up.\n","[----------] 4 tests from VulkanAPITest\n","[ RUN      ] VulkanAPITest.gru_mclareninputs_success\n","[       OK ] VulkanAPITest.gru_mclareninputs_success (1037 ms)\n","[ RUN      ] VulkanAPITest.gru_invalidinputs_exceptions\n","[       OK ] VulkanAPITest.gru_invalidinputs_exceptions (16 ms)\n","[ RUN      ] VulkanAPITest.gru_prepack_success\n","[       OK ] VulkanAPITest.gru_prepack_success (45 ms)\n","[ RUN      ] VulkanAPITest.gru_prepack_invalidinputs_exceptions\n","[       OK ] VulkanAPITest.gru_prepack_invalidinputs_exceptions (16 ms)\n","[----------] 4 tests from VulkanAPITest (1114 ms total)\n","[----------] Global test environment tear-down\n","[==========] 4 tests from 1 test case ran. (1114 ms total)\n","[  PASSED  ] 4 tests.\n","```\n","\n","Test result on MacOS (x86_64):\n","```\n","Running main() from gtest_main.cc\n","[==========] Running 4 tests from 1 test case.\n","[----------] Global test environment set-up.\n","[----------] 4 tests from VulkanAPITest\n","[ RUN      ] VulkanAPITest.gru_mclareninputs_success\n","[       OK ] VulkanAPITest.gru_mclareninputs_success (1012 ms)\n","[ RUN      ] VulkanAPITest.gru_invalidinputs_exceptions\n","[       OK ] VulkanAPITest.gru_invalidinputs_exceptions (40 ms)\n","[ RUN      ] VulkanAPITest.gru_prepack_success\n","[       OK ] VulkanAPITest.gru_prepack_success (99 ms)\n","[ RUN      ] VulkanAPITest.gru_prepack_invalidinputs_exceptions\n","[       OK ] VulkanAPITest.gru_prepack_invalidinputs_exceptions (39 ms)\n","[----------] 4 tests from VulkanAPITest (1190 ms total)\n","[----------] Global test environment tear-down\n","[==========] 4 tests from 1 test case ran. (1190 ms total)\n","[  PASSED  ] 4 tests.\n","```\n","\n","Reviewed By: SS-JIA\n","\n","Differential Revision: D34556940\n","\n","fbshipit-source-id: 79ed13e81b804521e7dc7c7c1a28404ced8d3100 \n","-----------------\n","Matches: [('(', '66580)')]\n","Use cub 1.15's latest scan-by-key algorithm to replace thrust for Embedding.cu and EmbeddingBag.cu (#66580)\n","\n","Summary:\n","\n","Reviewed By: mruberry\n","\n","Differential Revision: D34116388\n","\n","Pulled By: ngimel\n","\n","fbshipit-source-id: 2e8936ca7c10f96a8e7a5696248f56bf87290d6e\n","(cherry picked from commit 51cff8cb1de725bca52d5137b01b16d054b95f63)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73623)')]\n","[Static Runtime] Avoid index computation for `ProcessedNode::Output(i)` calls (#73623)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73623\n","\n","Currently, `ProcessedNode::Output` computes the output offset by adding `outputs_offset` to the given index of an output *at runtime* as follows:\n","\n","```\n","  // Output is readwrite\n","  IValue& Output(uint32_t i) {\n","    DCHECK(i < num_outputs());\n","    return values_[outputs_offset_ + i];\n","  }\n","```\n","\n","This change avoids this unnecessary runtime computation by adjusting `values_` pointer at the creation of `ProcessedNode`. A bonus of doing this is that we can now remove `ProcessedNode::outputs_offset_`. This removal doesn't compact the size of `ProcessedNode` with this change (still 48 bytes), but paves a road for further compaction.\n","\n","Test Plan: Existing tests\n","\n","Differential Revision: D34024340\n","\n","fbshipit-source-id: e3d9b5285933cd87aabb6f5a05739ab6fb7e79b3 \n","-----------------\n","Matches: [('(', '1175)'), (' ', '1129')]\n","Missed pr from upstream merge (#1175)\n","\n","Fixes #1129\n","\n","Thread predicates are missing in generating unswitch conditions. This PR collects thread predicates from unswitched expressions, merge them and append the merged one into the generated unswitch Bool val.\n","\n","The main new logic is the merging of thread predicates at: ThreadPredicateMap::mergeForUnswitch. Other changes are mostly minor cosmetic ones.\n","\n","Co-authored-by: Naoya Maruyama <naoyam@users.noreply.github.com>\n","Co-authored-by: jiej <jiej@nvidia.com> \n","-----------------\n","Matches: [('(', '1073)')]\n","Perf Tuning and Schedulers refactor (#1073) \n","-----------------\n","Matches: [('(', '1184)')]\n","Fix predicates and indexing for vectorization with unswitch and unroll. (#1184)\n","\n","Co-authored-by: Naoya Maruyama <nmaruyama@nvidia.com> \n","-----------------\n","Matches: [('(', '1190)')]\n","Benchmark refactoring, add backwards benchmarks. (#1190)\n","\n","Make sure benchmark sizes are built out and as consistent as possible. Add backwards benchmarks for BatchNorm, LayerNorm, and Softmax. \n","-----------------\n","Matches: [('(', '1157)')]\n","Print smem error info (#1157) \n","-----------------\n","Matches: [('(', '1163)')]\n","Fix computation of thread predicate with broadcast (#1163)\n","\n","* Fix computation of thread predicate with broadcast\n","\n","Previously, a broadcasted input resets a thread predicate of any other input. \n","-----------------\n","Matches: [('(', '1174)')]\n","Inline thread predicates even when unswitched (#1174)\n","\n","* Inline thread predicates even when unswitched \n","-----------------\n","Matches: [('(', '1152)')]\n","Explicitly track loops and IterDomains that do not contribute to indexing (#1152)\n","\n","Indices of unused loops are mapped to zero, so that fact is currently\n","used to find which loops are not used. This is fine for now, but not\n","if shift and unswitch are combined. With shift, a lower bound\n","position may need to be predicated as well, so that loop would get zero\n","as its index, even though the loop is used. To disambiguate this,\n","zero_loops and zero_domains are explicitly managed starting from\n","indexMapFromTV. \n","-----------------\n","Matches: [('(', '1156)')]\n","Fix invalid downcasting (#1156)\n","\n","Validation of allocations need to be done only for tensors, so\n","non-tensor allocations can be just skipped. \n","-----------------\n","Matches: [('(', '1158)')]\n","Use WARP_SIZE instead of 32 (#1158)\n","\n","* Use WARP_SIZE instead of 32 \n","-----------------\n","Matches: [('(', '1160)')]\n","Place unswitched shared memory allocations outside of unswitched domains (#1160)\n","\n","* Place unswitched shared memory allocations outside of unswitched domains\n","\n","In lower allocations, the position of allocation and initialization are\n","separately tracked. They are the same except with unswitched shared\n","memory allocations. \n","-----------------\n","Matches: [('(', '1177)')]\n","cleanup (#1177) \n","-----------------\n","Matches: [('(', '1159)')]\n","Prevent unused variable warning (#1159) \n","-----------------\n","Matches: [('(', '1161)')]\n","Allow setting contiguity of tensors (#1161)\n","\n","* Allow setting contiguity of tensors \n","-----------------\n","Matches: [('(', '1166)')]\n","Detect parallelization with predicated parallel types (#1166)\n","\n","\n","* Thread predicate map must be created before validating parallelization\n","\n","* Use the loop map to find corresponding axes in validating\n","parallelization between producers and consumers \n","-----------------\n","Matches: [('(', '1182)')]\n","Do not predicate non-exact parallel domains when generating unswitch predicates (#1182)\n","\n","* Predicating threadIdx/blockIdx at unswitch isn't necessary\n","\n","When generating unswitch predicates, maximum index values are used to\n","generate predicates at root domains, so it's redundant to predicate\n","threadIdx/blockIdx at leaf domains even for non-exact threading\n","dimensions. \n","-----------------\n","Matches: [('(', '1145)')]\n","Merging the shift-specific predicate logic into the main predicate logic (#1145)\n","\n","* Merge the predication logic for shift/gather into the main one\n","\n","One of the major changes needed to integrate the predicate logic for\n","shift/gather is to support predication at the start position of an\n","IterDomain. Because of that, there's a lot of \"start_xyz\" and\n","\"stop_xyz\".\n","\n","Another complexity comes from the extension to support unswitching with\n","shift/gather. In addition to start and stop predicates, existence of\n","halo means that the expressions unswitched at a domain may have\n","different halo sizes (or none at all), so picking just whatever\n","predicate per predicated root domain (and how it's parallelized) does\n","not work. The most naive approach would be to gather all of the\n","predicates for halo-extended root domains, but that's not efficient\n","since some would be redundant.\n","\n","What's done in this PR is to try to select the most restrictive\n","predicate by comparing the deviation from the baseline predicate.\n","Suppose one stop predicate is composed as \"x < extension\". With halo, it\n","would look like \"x + a < extension\", where \"a\" varies based on the halo\n","width of the predicated domain. When \"a\" is a static constant, we find\n","the maximum value and only use that predicate since that's the most\n","restrictive one. Start predicates are analyzed similarly as well. \n","-----------------\n","Matches: [('(', '1192)')]\n","Disable NVTX recording with PYTORCH_NVFUSER_DIABLE_NVTX (#1192)\n","\n","* Keep NVTX on by default. Use PYTORCH_NVFUSER_DISABLE_NVTX to disable it \n","-----------------\n","Matches: [('(', '1131)'), (' ', '1102'), (' ', '1102')]\n","Change indexing and predication to address non-exact threading dimensions (#1131)\n","\n","Fixes #1102 \n","\n","This PR implements the second approach mentioned in #1102 For example, indexing and predicates are changed from:\n","\n","```\n","      = T0[(((((nvfuser_index_t)blockIdx.x) * ((nvfuser_index_t)blockDim.y)) + ((nvfuser_index_t)threadIdx.y)) * T0.stride[0])]\n","```\n","\n","to:\n","\n","```\n"," = T0[(((((nvfuser_index_t)blockIdx.x) * 4) + ((nvfuser_index_t)threadIdx.y)) * T0.stride[0])]\n","```\n","\n","The use of `blockDim.y` is replaced by the extent of the second axis of `T0`, which is `4` in this case. This change only matters when a parallel type is not exact (in this case `TIDy`). \n","\n","The indexing change only needed to change `getExtent` in index_compute.cpp. However, we also need to predicate `threadIdx` and `blockIdx` to be smaller than IterDomain extents. That's implemented as `ParallelizedDomainPredicate` in predicate_compute.h. \n","-----------------\n","Matches: [('(', '1188)')]\n","Type Promotion and Special Number Test Cases (#1188)\n","\n","Create type_promotion tests for unary, binary, and ternary ops\n","* Rename test_data_compatibility to test_unary_ops \n","-----------------\n","Matches: [('(', '65137)'), ('(', '1170)'), ('(', '65137)')]\n","Revert \"Revert D30752939: [pytorch][PR] nvfuser update\" (#65137) (#1170)\n","\n","* Revert \"Revert D30752939: [pytorch][PR] nvfuser update\" (#65137)\n","\n","Summary:\n","This reverts commit 03389dc851db6f3ca52f9a4455ce2090c64a223d.\n","\n","Attempt again for PR: https://github.com/pytorch/pytorch/issues/63745\n","Fixes the windows build failure.\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/65137\n","\n","Reviewed By: seemethere, dzhulgakov, heitorschueroff\n","\n","Differential Revision: D30994556\n","\n","Pulled By: malfet\n","\n","fbshipit-source-id: f1925b6c5cc1a1a441a96499667c91e8dfc1b53d\n","\n","* review comments addressed\n","\n","* clang-tidy non-private member variables\n","\n","* clang-format\n","\n","* quick fix on skipping logic \n","-----------------\n","Matches: [('(', '1180)')]\n","softmax/backward dtype argument support (#1180)\n","\n","support dtype argument in softmax and softmax backward to accommodate the no-fusion issue with updated LTC IR \n","-----------------\n","Matches: [('(', '1186)')]\n","adding removeInplaceOperations pass for nvfuser (#1186) \n","-----------------\n","Matches: [('(', '66176)'), ('(', '1178)')]\n","Revert \"Revert D31227448: [pytorch][PR] fixing sorting in stride indices\" (#66176) (#1178)\n","\n","Summary:\n","enabling https://github.com/pytorch/pytorch/issues/63940\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/66176\n","\n","Reviewed By: ngimel\n","\n","Differential Revision: D31423920\n","\n","Pulled By: dzhulgakov\n","\n","fbshipit-source-id: 06b1e0f757f4fb5b31ee1fa464bcd689df919b9c \n","-----------------\n","Matches: [('(', '65064)'), ('(', '1179)')]\n","[JIT] Initialize CUDA context before launching fused kernel (#65064) (#1179)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/65064\n","\n","The problem appears when nvfuser is triggered from LazyTensor.\n","Because LT maintains its own thread pool, the thread used for the first-time\n","compilation does CUDA context initialization properly, but later\n","cached execution may use a different thread which does not have\n","a proper CUDA context.\n","\n","Test Plan: Imported from OSS\n","\n","Reviewed By: saketh-are\n","\n","Differential Revision: D31269691\n","\n","Pulled By: desertfire\n","\n","fbshipit-source-id: 384362025c087d61e8b625ff938379df283ef8b2\n","\n","Co-authored-by: Bin Bao <binbao@fb.com> \n","-----------------\n","Matches: [('(', '1176)')]\n","range-based for loop (#1176)\n","\n","code cleaning for clang-tidy \n","-----------------\n","Matches: [('(', '1118)')]\n","[WIP] Channels last refactor (#1118)\n","\n","Channels Last support in nvfuser\n","\n","Background:\n","To support channels last in nvfuser with optimal performance, we want to allow dimension collapsing in generated code on channels-last tensors, which greatly simplifies indexing.\n","Current API in codegen only allows dimensional collapsing on neighboring axes. The unfortunate thing is that memory format design in PyTorch is implicitly marked by strides, while the semantics meaning of axes remain unchanged. i.e. A 4d tensor with axes [N, C, H, W] would have the same shape in both format, while contiguous tensor carries strides [CHW, HW, W, 1] and channels-last tensor [HWC, 1, WC, C].\n","\n","Approach:\n","We identify input tensor in channels-last format and permute them to NHWC. This creates an inconsistency between codegen tensor and TorchScript tensor. Our parser handles and propagates memory format accordingly. I.e., consumes and produces channels-last inputs when it can, while transposes inputs to original format and output non-permuted outputs.\n","Fusion inputs/outputs in channels-last format is marked and permuted before/after fusion execution to ensure correctness on the interfacing between nvfuser and TorchScript.\n","\n"," add simple cpp test to ensure simplified indexing in generated code.\n","add python tests to verify nhwc fp16 inputs is handled properly. It has been handled in recent bfloat PR \n","-----------------\n","Matches: [('(', '1191)')]\n","Parser refactor (#1191) \n","-----------------\n","Matches: [('(', '1193)')]\n","issue 1189 repro and fix (#1193) \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Rename 'ITensorList' to 'ITensorListRef'. on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Update on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Update on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Remove structured kernels special casing in the codegen.\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Rebased to 'viable/strict.' on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Update on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Invoke constructor explicitly when translating. on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73350.'), (' ', '72414'), (' ', '72414,')]\n","Unmerge `cat` and `Tensor?[]` support (mistakenly removed). on \"Remove structured kernels special casing in the codegen.\"\n","\n","\n","This PR removes the special casing introduced in #73350.\n","\n","**What was this special casing for?**\n","It was introduced so that structured kernels could take advantage of `IList` from the\n","dispatcher. Basically, this special casing associated, for example, `Tensor[]` with\n","`const ITensorList &`, instead of the previous `TensorList`. The other kernels remained\n","using `TensorList`.\n","\n","**Why is it being removed?**\n","After #72414 and #72414, this special casing is no longer needed. Those 2 PRs modifies the\n","signatures of every kernel that has `Tensor[]` or `Tensor?[]` as arguments. Replacing them\n","by `ITensorList` and `IOptTensorRefList`, respectively. From that PR onwards, every kernel\n","is using the new API, so there's no need to special case it only for structured kernels.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73663)')]\n","[Quant] Qadd: Add qint8 support backed by xnnpack (#73663)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73663\n","\n","If built w/ USE_XNNPACK, for qint8 dtype, qadd is performed using xnnpack.\n","\n","Reviewed By: kimishpatel\n","\n","Differential Revision: D34470378\n","\n","fbshipit-source-id: 91383a03f35f482f2850dc68cacd36166d9b508a \n","-----------------\n","Matches: [('(', '73403)')]\n","[PyTorchEdge] Extend _save_for_mobile to support flatbuffer format (#73403)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73403\n","\n","Add an additional argument (with default to pickle untill flatbuffer rollout) to provide option to switch between formats\n","\n","Note: this is going to cause increase in the size of the libtorch or pytorch mobile build as flatbuffer and accompanying third party deps will be pulled in.\n","ghstack-source-id: 149917260\n","\n","Test Plan:\n","```\n","buck test //xplat/caffe2:test_lite_interpreter --config client.id=nuclide\n","\n","Parsing buck files: finished in 5.7 sec\n","Creating action graph: finished in 0.6 sec\n","[RE] Metadata: Session ID=[reSessionID-067d1335-9bb3-4c49-bcb4-bf2d21909f94]\n","[RE] Waiting on 0 remote actions. Completed 749 actions remotely, action cache hit rate: 0.00%.\n","Downloaded 3876/5582 artifacts, 18.91 Mbytes, 17.7% cache miss (for updated rules)\n","Building: finished in 02:03.7 min (100%) 5482/5482 jobs, 5429/5482 updated\n","  Total time: 02:10.1 min\n","Testing: finished in 07:03.7 min (82 PASS/0 FAIL)\n","BUILD SUCCEEDED\n","RESULTS FOR //xplat/caffe2:test_lite_interpreter\n","PASS    416.4s 82 Passed   0 Skipped   0 Failed   //xplat/caffe2:test_lite_interpreter\n","TESTS PASSED\n","\n","```\n","\n","Differential Revision: D34321403\n","\n","fbshipit-source-id: 8f5bd4cb1c37a9124f3673ad24325c3cd6946533 \n","-----------------\n","Matches: [('(', '73669)')]\n","[Quant] Qconv: Add qint8 support backed by xnnpack (#73669)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73669\n","\n","if built with USE_XNNPACK, support for quantized conv 1d, 2d w/ transpose and per_channel support for act dtype KQInt8 is added. For KQUInt8 act dtype qnnpack is used as before.\n","\n","Reviewed By: kimishpatel\n","\n","Differential Revision: D34470379\n","\n","fbshipit-source-id: 118ba8c81746d2e68a10f71dcec0adb7add34dcf \n","-----------------\n","Matches: [('(', '73669)')]\n","[Quant] Qconv: Add qint8 support backed by xnnpack (#73669)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73669\n","\n","if built with USE_XNNPACK, support for quantized conv 1d, 2d w/ transpose and per_channel support for act dtype KQInt8 is added. For KQUInt8 act dtype qnnpack is used as before.\n","\n","Differential Revision: https://www.internalfb.com/diff/D34470379?entry_point=27\n","\n","fbshipit-source-id: 7937f64315dffbfec36926074bf8d889bd32e05f \n","-----------------\n","Matches: [('(', '73672)')]\n","[Quant] Qlinear Add qint8 support backed by xnnpack (#73672)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73672\n","\n","If built w/ USE_XNNPACK, for qint8 dtype, qlinear is performed using xnnpack. For quint8 input dtype, use QNNPACK.\n","\n","Reviewed By: kimishpatel\n","\n","Differential Revision: D34022360\n","\n","fbshipit-source-id: 41af3172c802fb078620165f10cfefcf7c947c73 \n","-----------------\n","Matches: [('(', '73685)\"')]\n","Update on \"[Quant][test] Added test to check if fp16 packing->unpacking yields the same result as to(torch.float16).to(torch.float32) (#73685)\"\n","\n","\n","Summary:\n","\n","A test was added in test_quantized_op.py that checks whether the fp16 packing and subsequent unpacking of a given fp32 tensor produces the same result as to(torch.float16).to(torch.float32)\n","\n","Test Plan:\n","in pytorch main directory, execute\n","```\n","python test/test_quantization.py TestDynamicQuantizedOps.test_pack_unpack_fp16\n","```\n","\n","in pytorch main directory, execute\n","```\n","python test/test_quantization.py TestDynamicQuantizedOps.test_pack_unpack_fp16\n","```\n","\n","Reviewed By: jerryzh168\n","\n","Pulled By: dzdang\n","\n","fbshipit-source-id: 5da453e5db4801dde196424282140726c8a4ef1f\n","(cherry picked from commit ac8910e7feb4eebf677c99f287d48915165a87bf)\n","\n","Differential Revision: [D34599476](https://our.internmc.facebook.com/intern/diff/D34599476)\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73184')]\n","Catch overflows in calculating storage byte size\n","\n","Fixes #73184\n","\n","In the issue the output tensor's shape is [2, 4, 536870912, 536870912]\n","which results in a `numel()` slightly below the point of overflow.\n","When the storage is created it does `numel() * 8` which overflows and\n","a much smaller storage is allocated than required. \n","-----------------\n","Matches: [(' ', '73717')]\n","Update on \"[JIT] make RegisterCudaFuseGraph use TORCH_API instead of C10_EXPORT\"\n","\n","\n","I think this was causing multiple implementations of\n","RegisterCudaFuseGraph on windows. It looks like both torch_cpu.dll and\n","torch_python.dll were exporting RegisterCudaFuseGraph implementations,\n","so RegisterCudaFuseGraph::isRegistered() would refer to a _different_\n","static bool depending on whether the caller was in torch_cpu.dll or\n","torch_python.dll. See #73717 for a demonstration.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73717')]\n","[JIT] make RegisterCudaFuseGraph use TORCH_API instead of C10_EXPORT\n","\n","I think this was causing multiple implementations of\n","RegisterCudaFuseGraph on windows. It looks like both torch_cpu.dll and\n","torch_python.dll were exporting RegisterCudaFuseGraph implementations,\n","so RegisterCudaFuseGraph::isRegistered() would refer to a _different_\n","static bool depending on whether the caller was in torch_cpu.dll or\n","torch_python.dll. See #73717 for a demonstration.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73744)')]\n","[draft] add script to generate test model for ops (#73744)\n","\n","Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/73744\n","\n","Differential Revision: D34518432\n","\n","fbshipit-source-id: 286078fd94bc165ec8b07dbe7c37506efb1a7bc6 \n","-----------------\n","Matches: [('(', '73745)')]\n","[NS] Mark output logger impure to avoid being removed in acc tracer (#73745)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73745\n","\n","Mark output logger as impure, which will help prevent it and the shadow ops from being removed in acc tracer.\n","\n","Test Plan: Tested in N1611591\n","\n","Reviewed By: jerryzh168\n","\n","Differential Revision: D34616990\n","\n","fbshipit-source-id: 96cc91c5f1b3162854c5c127f98044cb4dbaed61 \n","-----------------\n","Matches: [('(', '73746)')]\n","[OSS] add script to generate test models for mobile (#73746)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73746\n","\n","Added some scripts to generate 4 models for OSS mobile test\n","\n","- tensor related ops\n","- math ops\n","- neural net related ops\n","- random sampling ops\n","\n","So far these models covered around half of root ops used in production.\n","Will continue to iterate for more ops.\n","\n","Also updated iOS test to run these test models.\n","\n","Test Plan:\n","all tests pass on iOS simulator\n","\n","{F708807282}\n","\n","Reviewed By: xta0\n","\n","Differential Revision: D34616936\n","\n","fbshipit-source-id: b01850d84844212056bef0465cb3ca557cad8c68 \n","-----------------\n","Matches: [('(', '73763)')]\n","[FX lowering] Modify replace_all_uses_with to allowing filtering of nodes to update; use it to (#73763)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73763\n","\n","The test that is enabled generates a graph as such:\n","\n","```\n","linear_25 --> sigmoid_14 --> output_1\n","         \\--> output_2\n","```\n","Before this diff, (unpadding) layout_transform nodes would be added as follows:\n","\n","```\n","linear_25 --> layout_xform1 --> sigmoid_14 --> layout_xform2--> output_1\n","                           \\--> output_2\n","```\n","This causes an assertion to fail for the sigmoid node where the input and output types\n","don't match due to padding differences.\n","\n","This diff modifies the replacement algorithm to not affect users of an output's parent node\n","when the user requires padded inputs. This yields the following graph instead:\n","\n","```\n","linear_25 --> sigmoid_14 --> layout_xform2--> output_1\n","         \\--> layout_xform1 --> output_2\n","```\n","\n","Test Plan: Manually and CI\n","\n","Reviewed By: jfix71, dborkovic\n","\n","Differential Revision: D34623590\n","\n","fbshipit-source-id: eabb03017844be182dbe6abcf4fb91dc8d67cb88 \n","-----------------\n","Matches: [('(', '73810)')]\n","regenerate flatbuffer header (#73810)\n","\n","Summary:\n","Update flatbuffer generated header and add it to ignore for clang format\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73810\n","\n","Test Plan: CI\n","\n","Reviewed By: iseeyuan\n","\n","Differential Revision: D34652217\n","\n","Pulled By: qihqi\n","\n","fbshipit-source-id: 9a7daac921da7bae1bdd761942c2489c6902453a \n","-----------------\n","Matches: [('(', '73815)')]\n","[fx/graph_drawer] Add skip_node_names_in_args option, default to True (#73815)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73815\n","\n","Add `skip_node_names_in_args` (default=`True`) which will skip including node names in args/kwargs during graph drawing.\n","\n","Test Plan:\n","Default (`skip_node_names_in_args=True`):\n","\n","{F707455583}\n","\n","Vs. `skip_node_names_in_args=False`:\n","\n","{F707046375}\n","\n","Reviewed By: wushirong\n","\n","Differential Revision: D34659144\n","\n","fbshipit-source-id: 918c348c853a1931522567851085f4122dd13141 \n","-----------------\n","Matches: [('(', '73855)')]\n","[Profiler] Prefer TSC to wall clock when available (#73855)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73855\n","\n","Calling the clock is one of the most expensive parts of profiling. We can reduce the profiling overhead by using `rdtsc` instead. The tradeoff is that we have to measure and convert. (shift and scale)\n","\n","Test Plan: I added a cpp unit test with *very* aggressive anti-flake measures. I also ran the overhead benchmark (9 replicates) with `--stressTestKineto` (0.94 -> 0.89 us) and `--stressTestKineto --kinetoProfileMemory` (1.27 -> 1.17 us)\n","\n","Reviewed By: chaekit\n","\n","Differential Revision: D34231071\n","\n","fbshipit-source-id: a7c2e9a05d5f1328444231dc439570afe8b82fc8 \n","-----------------\n","Matches: [('(', '73858)')]\n","[Kineto] Manual Submodule Update (#73858)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73858\n","\n","The newer version of Kineto has changes to handle generic activities (such as RocTracer generic activities), so we can remove the older USE_KINETO_UPDATED macro and implementation of flow.linkedActivity.\n","\n","This patch should bring Kineto back in sync on PyTorch CI.\n","\n","Test Plan: PyTorch OSS CI needs to pass for this submodule update of third_party/kineto repo. With ciflow/cuda enabled.\n","\n","Reviewed By: chaekit\n","\n","Differential Revision: D34689078\n","\n","Pulled By: aaronenyeshi\n","\n","fbshipit-source-id: 87749f7c0e3a5bd585525e90196fd661500e3746 \n","-----------------\n","Matches: [('(', '73872)')]\n","[vulkan] Enable Pytorch Vulkan to build in FBCode (#73872)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73872\n","\n","This diff adds an equivalent target for [`aten_vulkan`](https://fburl.com/code/h9ybej5u) in FBCode as the `ATen-vulkan` target. This diff simply creates equivalent fbcode targets for all the xplat targets needed to build `aten_vulkan`:\n","\n","The following targets in `xplat/caffe2` have had equivalent targets created in `fbcode/caffe2/aten`\n","* `aten_vulkan_glsl_src_path`\n","  * filegroup containing all Vulkan glsl files\n","* `gen_aten_vulkan_spv_lib`\n","  * python library containing script to generate vulkan spv files\n","* `gen_aten_vulkan_spv_bin`\n","  * python binary wrapping the above target\n","* `gen_aten_vulkan_spv`\n","  * genrule to execute the above python script and create C++ headers containing the SPIR-V shader code\n","* `generated_aten_headers_vulkan`\n","  * C++ library that points to the generated SPIR-V headers from above\n","* `aten_vulkan`\n","  * Contains the Pytorch Vulkan backend\n","\n","FBCode targets have also been added for:\n","* `Vulkan-Headers` which contains Vulkan API function signatures\n","* `vulkan_wrapper` which loads the vulkan library\n","* `dotslash:glslc` which wraps the glsl compiler in a target that can be executed by genrules\n","\n","Test Plan:\n","Try building the new `ATen-vulkan` target:\n","\n","```\n","cd fbsource/fbcode/caffe2/aten\n","buck build :ATen-vulkan\n","```\n","\n","Also tested in the next diff which tries to use this target in a Python script in FBCode.\n","\n","Reviewed By: beback4u\n","\n","Differential Revision: D34647445\n","\n","fbshipit-source-id: 7512a2072d0d109b1affd152982f03fbbb4940a0 \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_factor\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for `lu_factor`. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_factor\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for `lu_factor`. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('\\r\\n\\r\\n', '#')]\n","Update on \"Update and improve the heuristics for linalg.lu_solve\"\n","\n","\n","This PR adds getrf_cublas to the functions considered in the heuristics\n","for lu_solve. It also updates the heuristics of the function.\n","\n","## Benchmark\n","\n","I'm omitting form the benchmarks the looped versions of the functions as they are much slower than the non-looped ones. The only exception to this is cusolver's looped variant, which is faster when applied to a batch of size one.\n","\n","<details>\n","<summary>\n","Benchmark Results\n","</summary>\n","\n","```\n","[------------------------------------------------- linalg.lu_factor CUDA -------------------------------------------------]                                                                                          \n","                                          |  lu_factor_heuristic  |  lu_factor_magma_batched  |  lu_factor_cusolver_batched                                                                                          \n","1 threads: ----------------------------------------------------------------------------------------------------------------                                                                                          \n","      shape torch.Size([1, 1, 1])         |            26         |              47           |                26                                                                                                    \n","      shape torch.Size([2, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([4, 1, 1])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 1, 1])         |            20         |              38           |                18                                                                                                    \n","      shape torch.Size([16, 1, 1])        |            20         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 1, 1])        |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 1, 1])        |            18         |              39           |                17                                                                                                    \n","      shape torch.Size([128, 1, 1])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 1, 1])       |            18         |              39           |                18                                                                                                    \n","      shape torch.Size([1024, 1, 1])      |            18         |              40           |                18                                                                                                    \n","      shape torch.Size([1, 2, 2])         |            18         |              38           |                17                                                                                                    \n","      shape torch.Size([2, 2, 2])         |            17         |              37           |                17                                                                                                    \n","      shape torch.Size([4, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([8, 2, 2])         |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([16, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([32, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([64, 2, 2])        |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([128, 2, 2])       |            17         |              38           |                17                                                                                                    \n","      shape torch.Size([512, 2, 2])       |            17         |              39           |                17                                                                                                    \n","      shape torch.Size([1024, 2, 2])      |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([1, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([2, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([4, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([8, 8, 8])         |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([16, 8, 8])        |            17         |              41           |                17                                                                                                    \n","      shape torch.Size([32, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([64, 8, 8])        |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([128, 8, 8])       |            17         |              40           |                17                                                                                                    \n","      shape torch.Size([512, 8, 8])       |            17         |              42           |                17                                                                                                    \n","      shape torch.Size([1024, 8, 8])      |            17         |              44           |                17                                                                                                    \n","      shape torch.Size([1, 16, 16])       |            24         |              44           |                18                                                                                                    \n","      shape torch.Size([2, 16, 16])       |            18         |              44           |                18                                                                                                    \n","      shape torch.Size([4, 16, 16])       |            18         |              45           |                18          \n","      shape torch.Size([8, 16, 16])       |            19         |              44           |                19          \n","      shape torch.Size([16, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([32, 16, 16])      |            20         |              45           |                20          \n","      shape torch.Size([64, 16, 16])      |            20         |              44           |                20          \n","      shape torch.Size([128, 16, 16])     |            20         |              45           |                20          \n","      shape torch.Size([512, 16, 16])     |            28         |              50           |                28          \n","      shape torch.Size([1024, 16, 16])    |            41         |              59           |                41          \n","      shape torch.Size([1, 32, 32])       |            58         |              50           |                56          \n","      shape torch.Size([2, 32, 32])       |            56         |              50           |                56          \n","      shape torch.Size([4, 32, 32])       |            56         |              50           |                57          \n","      shape torch.Size([8, 32, 32])       |            60         |              50           |                60          \n","      shape torch.Size([16, 32, 32])      |            60         |              51           |                60          \n","      shape torch.Size([32, 32, 32])      |           247         |              51           |                61          \n","      shape torch.Size([64, 32, 32])      |           233         |              51           |                63          \n","      shape torch.Size([128, 32, 32])     |           236         |              53           |                66          \n","      shape torch.Size([512, 32, 32])     |           268         |              97           |               193          \n","      shape torch.Size([1024, 32, 32])    |           317         |             167           |               333          \n","      shape torch.Size([1, 64, 64])       |           131         |             216           |                99          \n","      shape torch.Size([2, 64, 64])       |            99         |             220           |                99          \n","      shape torch.Size([4, 64, 64])       |            99         |             225           |               101          \n","      shape torch.Size([8, 64, 64])       |           101         |             225           |               102          \n","      shape torch.Size([16, 64, 64])      |           107         |             230           |               108          \n","      shape torch.Size([32, 64, 64])      |           440         |             235           |               126          \n","      shape torch.Size([64, 64, 64])      |           447         |             240           |               155          \n","      shape torch.Size([128, 64, 64])     |           470         |             289           |               240          \n","      shape torch.Size([512, 64, 64])     |           793         |             678           |              1180          \n","      shape torch.Size([1024, 64, 64])    |          1000         |            1300           |              2112          \n","      shape torch.Size([1, 128, 128])     |           296         |             482           |               309          \n","      shape torch.Size([2, 128, 128])     |           308         |             499           |               307          \n","      shape torch.Size([4, 128, 128])     |           311         |             510           |               310          \n","      shape torch.Size([8, 128, 128])     |           314         |             522           |               314          \n","      shape torch.Size([16, 128, 128])    |           334         |             541           |               334          \n","      shape torch.Size([32, 128, 128])    |           770         |             591           |               467          \n","      shape torch.Size([64, 128, 128])    |           860         |             694           |               733          \n","      shape torch.Size([128, 128, 128])   |          1040         |             925           |              1980          \n","      shape torch.Size([512, 128, 128])   |          2883         |            2809           |             11000          \n","      shape torch.Size([1024, 128, 128])  |          5421         |            5430           |             22360          \n","      shape torch.Size([1, 256, 256])     |          1310         |            1109           |              1556          \n","      shape torch.Size([2, 256, 256])     |          1360         |            1150           |              1560          \n","      shape torch.Size([4, 256, 256])     |          1390         |            1188           |              1569          \n","      shape torch.Size([8, 256, 256])     |          1440         |            1250           |              1604          \n","      shape torch.Size([16, 256, 256])    |          1550         |            1390           |              1850          \n","      shape torch.Size([32, 256, 256])    |          1750         |            1620           |              3332          \n","      shape torch.Size([64, 256, 256])    |          2327         |            2246           |              6700          \n","      shape torch.Size([128, 256, 256])   |          3697         |            3638           |             19100          \n","      shape torch.Size([512, 256, 256])   |         12530         |           12500           |             87300          \n","      shape torch.Size([1024, 256, 256])  |         24380         |           24420           |            176000          \n","```\n","\n","</details>\n","\n","To generate the results below, I put the backend I wanted to test at the beginning of the function `lu_solve_kernel`, followed by a `return;`. Then I run the following script, changing the variable `name`. For the `lu_solve unpack+solve_triangular`, I also changed the `stmt` variable (uncomenting the commented one)\n","<details>\n","<summary>\n","Benchmarking script\n","</summary>\n","\n","```python\n","import torch\n","import pickle\n","import itertools\n","from functools import partial\n","from torch.utils.benchmark import Timer, Compare\n","\n","benchmark_name = \"linalg.lu_factor CUDA\"\n","name = \"magma_batched\"\n","label = \"lu_factor_{}\".format(name)\n","shapes = [1, 2, 8, 16, 32, 64, 128, 256]\n","batches = [(1,), (2,), (4,), (8,), (16,), (32,), (64,), (128,), (512,), (1024,)]\n","results = []\n","make_arg = partial(torch.randn, dtype=torch.float32, device=\"cuda\")\n","\n","\n","for n, batch in itertools.product(shapes, batches):\n","    A = make_arg(batch + (n, n))\n","    print(A.shape)\n","    stmt = \"torch.linalg.lu_factor_ex(A)\"\n","    timer = Timer(stmt,\n","                  globals=globals(),\n","                  label=benchmark_name,\n","                  description=label,\n","                  sub_label=f\"shape {A.shape}\",\n","                  num_threads=1)\n","    results.append(timer.blocked_autorange())\n","\n","    # Test\n","    LU, pivots = torch.linalg.lu_factor(A)\n","    P, L, U = torch.lu_unpack(LU, pivots)\n","    assert torch.allclose(P @ L @ U, A, rtol=1e-2, atol=1e-3)\n","\n","\n","compare = Compare(results)\n","compare.trim_significant_figures()\n","compare.print()\n","\n","with open(f\"{label}.pickle\", 'wb') as f:\n","    pickle.dump(results, f)\n","```\n","\n","</details>\n","\n","See https://github.com/pytorch/pytorch/pull/72935#issue-1140501705 for the script to join the results.\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73879)')]\n","[pyper] to + lengths_to_offsets (#73879)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73879\n","\n","Fuse the following pattern:\n","```\n","%1994 : Tensor = aten::to(%getattr_78.1, %188, %189, %189) # <eval_with_key>.50:11:0\n","%1995 : Tensor = fb::lengths_to_offsets(%1994, %190) # /mnt/xarfuse/uid-1994\n","```\n","\n","This pattern is applied after all the applicable clip_ranges+gather_ranges patterns\n","\n","Additional context in https://fb.quip.com/DSCbAozMBwUi\n","\n","Test Plan:\n","> ./caffe2/caffe2/fb/predictor/scripts/run_disagg_model_benchmarks.sh 321004917 27 /data/users/ansha/tmp/ads_tail sr_only\n","\n","~0.007ms overall reduction in tail model runtime\n","(321004917_27 oemae_long_attr_win_2d_7d_aux_model)\n","\n","**Local  (25 fused nodes)**\n","Before: 2.04ms/iter\n","      0.0112739 ms.   0.543996%. fb::lengths_to_offsets (31 nodes, out variant)\n","     0.00805597 ms.   0.388722%. static_runtime::to_maybe_copy_out (30 nodes, out variant)\n","\n","After: 1.96256ms/iter\n","      0.0100853 ms.   0.498655%. fb::to_lengths_to_offsets (25 nodes, out variant)\n","     0.00328385 ms.   0.157536%. fb::lengths_to_offsets (6 nodes, out variant)\n","     0.00239722 ms.   0.115002%. static_runtime::to_maybe_copy_out (5 nodes, out variant)\n","\n","**Local_RO  (43 fused nodes)**\n","Before: 0.11427\n","      0.0110696 ms.    9.42255%. fb::lengths_to_offsets (43 nodes, out variant)\n","     0.00638323 ms.    5.43349%. static_runtime::to_maybe_copy_out (43 nodes, out variant)\n","After: 0.112098ms/iter\n","       0.014206 ms.    12.6795%. fb::to_lengths_to_offsets (43 nodes, out variant)\n","\n","**Remote_RO (17 fused nodes)**\n","Before: 0.24\n","      0.0534883 ms.    23.0586%. static_runtime::to_maybe_copy_out (136 nodes, out variant)\n","     0.00216992 ms.   0.935446%. fb::lengths_to_offsets (17 nodes, out variant)\n","After: 0.240225\n","      0.0525392 ms.    23.2864%. static_runtime::to_maybe_copy_out (119 nodes, out variant)\n","     0.00265347 ms.    1.17607%. fb::to_lengths_to_offsets (17 nodes, out variant)\n","\n","Remote_Other (3 fused nodes)\n","Not much affect\n","\n","Differential Revision: D34696255\n","\n","fbshipit-source-id: d9b07a9f7f3b3ec83584305295de5cdad538abc9 \n","-----------------\n","Matches: [('(', '73897)')]\n","[PyTorch Distributed] Add debug hint for NCCL async system error (#73897)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73897\n","\n","add a debug hint that async system error can be caused by unexpected exit of\n","a remote process if not an actual network issue. For example, the exit of the remote process\n","can cause a closed network connection error at a local process. The hint helps to direct\n","the debug focus to the remote process.\n","\n","Test Plan: unit tests\n","\n","Reviewed By: pritamdamania87, rohan-varma\n","\n","Differential Revision: D34702348\n","\n","fbshipit-source-id: 85ccebc25e4c3a685dfb7d2bc2d981778cd08cd7 \n","-----------------\n","Matches: [('(', '73899)')]\n","[Static Runtime] Add native op support for `aten::len` (#73899)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73899\n","\n","This change adds native op wrappers to Static Runtime as appears in JIT (https://www.internalfb.com/code/fbsource/[429d233b9beb5e6f60df7304b792e2ff332f6ecd]/fbcode/caffe2/torch/csrc/jit/runtime/register_prim_ops.cpp?lines=613 , search for \"aten::len\" in that file).\n","\n","Test Plan: Added unittests, \"StaticRuntime.LenWith*\", and confirmed they are passing with `V0307 17:39:39.817956 3516654 impl.cpp:1792] Switch to native impl for node: %2 : int = aten::len(%input.1)` per added unittest: P485159811\n","\n","Reviewed By: mikeiovine\n","\n","Differential Revision: D34705231\n","\n","fbshipit-source-id: c5635c3a80252cdf52ec4c2e8910f835a11ba0f6 \n","-----------------\n","Matches: [('(', '73945)')]\n","[Static Runtime] Add out variant wrapper for aten::ones_like (#73945)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73945\n","\n","This change adds add out variant wrapper for aten::ones_like.\n","\n","Test Plan:\n","- Added a unittest.\n","- Checked that the op execution got switched to its added out variant (P485330978).\n","\n","Reviewed By: hlu1\n","\n","Differential Revision: D34727057\n","\n","fbshipit-source-id: f1678fa4a3b4d87673010b86d3b4aa461af76117 \n","-----------------\n","Matches: [('(', '73946)')]\n","[Static Runtime] Add out variant wrapper for aten::zeros (#73946)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73946\n","\n","This change adds an out variant wrapper for aten::zeros.\n","\n","Test Plan:\n","- Added a unittest.\n","\n","- Confirmed that the added out variant gets executed by the unittest (P485324923).\n","\n","Reviewed By: mikeiovine\n","\n","Differential Revision: D34725843\n","\n","fbshipit-source-id: 884eac30d638e359544f6a0199ecbb2cfb394b46 \n","-----------------\n","Matches: [('(', '73951)'), (' ', '69881.')]\n","[PyTorch] Add safe peek and drop for stack in codegen (#73951)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73951\n","\n","A follow up PR for #69881. Basically we see issues such as trying to peek 6 elements in a stack with only 1 element, then trying to drop 6 elements from the same stack. Here I'm adding a small util to prevent these issues.\n","\n","Example:\n","\n","```\n","randn_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\n","```\n","And the stack only contains `self` Tensor.\n","\n","Test Plan: Added unit test\n","\n","Differential Revision: D34730802\n","\n","fbshipit-source-id: 34909fb367c76e0eb20746a6a36bd545a0749522 \n","-----------------\n","Matches: [(' ', '73971')]\n","Update on \"ci: Fix cudatoolkit issue, make docker builds testable\"\n","\n","\n","nightly docker builds were failing due to cudatoolkit=11.3.0 not\n","actually being available using conda.\n","\n","This remedies that and adds the ability to actually test this workflow\n","in a pull request\n","\n","Resolves #73971 \n","\n","Signed-off-by: Eli Uriegas <eliuriegasfb.com>\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73971')]\n","Update on \"ci: Fix cudatoolkit issue, make docker builds testable\"\n","\n","\n","nightly docker builds were failing due to cudatoolkit=11.3.0 not\n","actually being available using conda.\n","\n","This remedies that and adds the ability to actually test this workflow\n","in a pull request\n","\n","Resolves #73971 \n","\n","Signed-off-by: Eli Uriegas <eliuriegasfb.com>\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [(' ', '73971')]\n","Update on \"ci: Fix cudatoolkit issue, make docker builds testable\"\n","\n","\n","nightly docker builds were failing due to cudatoolkit=11.3.0 not\n","actually being available using conda.\n","\n","This remedies that and adds the ability to actually test this workflow\n","in a pull request\n","\n","Resolves #73971 \n","\n","Signed-off-by: Eli Uriegas <eliuriegasfb.com>\n","\n","[ghstack-poisoned] \n","-----------------\n","Matches: [('(', '73982)')]\n","(torchx/elastic) honor NCCL_ASYNC_ERROR_HANDLING set from the env var (#73982)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73982\n","\n","Currently there is no way for users using torchelastic to override NCCL_ASYNC_ERROR_HANDLING=0. This PR enables this.\n","\n","Test Plan:\n","Added unittests\n","\n","Manual testing\n","```\n","$ torchx run fb.dist.ddp -- --img torchx_examples -m print_env_vars.py --env NCCL_ASYNC_ERROR_HANDLING=0\n","```\n","\n","Validated the NCCL_ASYNC_ERROR_HANDLING in the process running `print_env_vars.py` is indeed `0`.\n","\n","Reviewed By: mannatsingh, aivanou\n","\n","Differential Revision: D34765786\n","\n","fbshipit-source-id: f4cb5623aa49d9d40d509c9c01a293276c7b8ee6 \n","-----------------\n","Matches: [('(', '73990)')]\n","[Static Runtime] Fix a bug that `aten::full` reuses a tensor that does not match requested one (#73990)\n","\n","Summary:\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73990\n","\n","This change fixes a bug that `aten::fill` reuses a previously allocated tensor that does not match requested one when arguments to `aten::fill` are dynamically changed.\n","\n","This fix is applied to multiple other out variant wrappers added to Static Runtime, and their fixes are following.\n","\n","Test Plan: - Added a unittest.\n","\n","Differential Revision: D34768718\n","\n","fbshipit-source-id: 098bc3c09b2fe45dac6b883ef98d6a84824878f1 \n","-----------------\n","Matches: [('(', '73996)')]\n","Run lazy tensor codegen in generate_code.py (#73996)\n","\n","Summary:\n","Hooks into existing autograd codegen script (generate_code.py) to take advantage of its integrations into buck/cmake/bazel.\n","\n","Adds a new option (--gen_lazy_ts_backend) to. generate_code.py, calling this from CMake OSS build and fbcode build, but not from other internal xplat/ovrsource builds (these could be opted in later)\n","\n","Bazel support is added in a later diff.\n","\n","Includes one generated file (torch/csrc/lazy/generated/LazyIr.h) in a unit test (test/cpp/lazy/test_ir.cpp) to partially verify the generator is working, but does not compile the remaining output sources from the generator yet as they depend on other files not yet landed from lazy_tensor_staging branch.\n","\n","Pull Request resolved: https://github.com/pytorch/pytorch/pull/73996\n","\n","Test Plan: OSS/internal CI - verify all builds are working and test_ir.cpp compiles LazyIr.h\n","\n","Reviewed By: ezyang\n","\n","Differential Revision: D34408536\n","\n","fbshipit-source-id: b7d46d817b3ed3c56108d65bbf052ed73b4e0827 \n","-----------------\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"CMOz6cb0gnzp"},"execution_count":null,"outputs":[]}]}